{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%load_ext tensorboard\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# placeholder for parameter\n",
    "###########################\n",
    "\n",
    "params = {\n",
    "    \"env_name\": \"CartPole-v0\",\n",
    "    \"exp_name\": \"sb_no_rtg_dsa\",\n",
    "    \"num_agent_train_steps_per_iter\": 1,\n",
    "    \"n_iter\": 10,\n",
    "    \"reward_to_go\": True,\n",
    "    \"nn_baseline\": True,\n",
    "    \"dont_standardize_advantages\": True,\n",
    "    \"batch_size\": 1000,\n",
    "    \"eval_batch_size\": 500,\n",
    "    \"train_batch_size\":1000,\n",
    "    \"discount\": 1.0,\n",
    "    \"n_layers\": 2,\n",
    "    \"size\": 64,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"video_log_freq\": 5,\n",
    "    \"scalar_log_freq\": 1,\n",
    "    \"use_gpu\": True,\n",
    "    \"which_gpu\":0,\n",
    "    \"max_replay_buffer_size\":1000000,\n",
    "    \"seed\":1\n",
    "}\n",
    "\n",
    "agent_params = {\n",
    "            'n_layers': params['n_layers'],\n",
    "            'size': params['size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'max_replay_buffer_size': params['max_replay_buffer_size'],\n",
    "            }\n",
    "\n",
    "params['agent_params'] = agent_params\n",
    "\n",
    "# params for saving rollout videos to tensorboard\n",
    "MAX_NVIDEO = 2\n",
    "MAX_VIDEO_LEN = 40\n",
    "\n",
    "log_video = True\n",
    "log_metric = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the gym environment\n",
    "seed = params['seed']\n",
    "env = gym.make(params['env_name'])\n",
    "env.seed(seed)\n",
    "\n",
    "# Maximum length for episodes\n",
    "params['ep_len'] = env.spec.max_episode_steps\n",
    "\n",
    "# Is this env continuous, or self.discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "params['agent_params']['discrete'] = discrete\n",
    "\n",
    "# Observation and action sizes\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "params['agent_params']['ac_dim'] = ac_dim\n",
    "params['agent_params']['ob_dim'] = ob_dim\n",
    "\n",
    "fps = env.env.metadata['video.frames_per_second']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Perhaps a deque is better\n",
    "##This function organise replay buffer for SARS\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_size=1000000):\n",
    "\n",
    "        self.max_size = max_size\n",
    "        self.paths = []\n",
    "        self.obs = None\n",
    "        self.acs = None\n",
    "        self.concatenated_rews = None\n",
    "        self.unconcatenated_rews = None\n",
    "        self.next_obs = None\n",
    "        self.terminals = None\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.obs:\n",
    "            return self.obs.shape[0]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def add_rollouts(self, paths):\n",
    "\n",
    "        # add new rollouts into our list of rollouts\n",
    "        for path in paths:\n",
    "            self.paths.append(path)\n",
    "\n",
    "        # convert new rollouts into their component arrays, and append them onto our arrays\n",
    "        observations, actions, next_observations, terminals, concatenated_rews, unconcatenated_rews = self.convert_listofrollouts(paths)\n",
    "\n",
    "        if self.obs is None:\n",
    "            self.obs = observations[-self.max_size:]\n",
    "            self.acs = actions[-self.max_size:]\n",
    "            self.next_obs = next_observations[-self.max_size:]\n",
    "            self.terminals = terminals[-self.max_size:]\n",
    "            self.concatenated_rews = concatenated_rews[-self.max_size:]\n",
    "            self.unconcatenated_rews = unconcatenated_rews[-self.max_size:]\n",
    "        else:\n",
    "            self.obs = np.concatenate([self.obs, observations])[-self.max_size:]\n",
    "            self.acs = np.concatenate([self.acs, actions])[-self.max_size:]\n",
    "            self.next_obs = np.concatenate([self.next_obs, next_observations])[-self.max_size:]\n",
    "            self.terminals = np.concatenate([self.terminals, terminals])[-self.max_size:]\n",
    "            self.concatenated_rews = np.concatenate([self.concatenated_rews, concatenated_rews])[-self.max_size:]\n",
    "            if isinstance(unconcatenated_rews, list):\n",
    "                self.unconcatenated_rews += unconcatenated_rews\n",
    "            else:\n",
    "                self.unconcatenated_rews.append(unconcatenated_rews)\n",
    "            \n",
    "    def convert_listofrollouts(self, paths):\n",
    "        \"\"\"\n",
    "            Take a list of rollout dictionaries\n",
    "            and return separate arrays,\n",
    "            where each array is a concatenation of that array from across the rollouts\n",
    "        \"\"\"\n",
    "        observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "        actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "        next_observations = np.concatenate([path[\"next_observation\"] for path in paths])\n",
    "        terminals = np.concatenate([path[\"terminal\"] for path in paths])\n",
    "        concatenated_rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
    "        unconcatenated_rewards = [path[\"reward\"] for path in paths]\n",
    "        return observations, actions, next_observations, terminals, concatenated_rewards, unconcatenated_rewards\n",
    "    \n",
    "    \n",
    "    def sample_random_rollouts(self, num_rollouts):\n",
    "        rand_indices = np.random.permutation(len(self.paths))[:num_rollouts]\n",
    "        return self.paths[rand_indices]\n",
    "\n",
    "    def sample_recent_rollouts(self, num_rollouts=1):\n",
    "        return self.paths[-num_rollouts:]\n",
    "\n",
    "    ########################################\n",
    "    ########################################\n",
    "\n",
    "    def sample_random_data(self, batch_size):\n",
    "\n",
    "        assert self.obs.shape[0] == self.acs.shape[0] == self.concatenated_rews.shape[0] == self.next_obs.shape[0] == self.terminals.shape[0]\n",
    "        rand_indices = np.random.permutation(self.obs.shape[0])[:batch_size]\n",
    "        return self.obs[rand_indices], self.acs[rand_indices], self.concatenated_rews[rand_indices], self.next_obs[rand_indices], self.terminals[rand_indices]\n",
    "\n",
    "    def sample_recent_data(self, batch_size=1, concat_rew=True):\n",
    "\n",
    "        if concat_rew:\n",
    "            return self.obs[-batch_size:], self.acs[-batch_size:], self.concatenated_rews[-batch_size:], self.next_obs[-batch_size:], self.terminals[-batch_size:]\n",
    "        else:\n",
    "            num_recent_rollouts_to_return = 0\n",
    "            num_datapoints_so_far = 0\n",
    "            index = -1\n",
    "            while num_datapoints_so_far < batch_size:\n",
    "                recent_rollout = self.paths[index]\n",
    "                index -=1\n",
    "                num_recent_rollouts_to_return +=1\n",
    "                num_datapoints_so_far += get_pathlength(recent_rollout)\n",
    "            rollouts_to_return = self.paths[-num_recent_rollouts_to_return:]\n",
    "            observations, actions, next_observations, terminals, concatenated_rews, unconcatenated_rews = self.convert_listofrollouts(rollouts_to_return)\n",
    "            return observations, actions, unconcatenated_rews, next_observations, terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(agent_params['max_replay_buffer_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_dim = agent_params['ac_dim']\n",
    "ob_dim = agent_params['ob_dim']\n",
    "n_layers = agent_params['n_layers']\n",
    "size = agent_params['size']\n",
    "discrete = agent_params['discrete']\n",
    "learning_rate = agent_params['learning_rate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, ob_dim, size, ac_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(ob_dim, size)\n",
    "        self.fc2 = torch.nn.Linear(size, size)\n",
    "        self.fc3 = torch.nn.Linear(size, ac_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "model = Net(ob_dim, size, ac_dim)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0609,  0.1366],\n",
       "        [-0.0638,  0.1331],\n",
       "        [-0.0605,  0.1363]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor([env.reset(), env.step(1)[0], env.step(0)[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor([env.reset()])).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Policy\n",
    "##################\n",
    "\n",
    "class BasePolicy:\n",
    "    def get_action(self, obs):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    \n",
    "\n",
    "class Policy(BasePolicy):\n",
    "    \n",
    "    def __init__(self, model, env, params, gamma, learning_rate):\n",
    "        super().__init__()\n",
    "        self.modelx = model\n",
    "        self.optimizer = torch.optim.Adam(self.modelx.parameters(), lr=learning_rate)\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.nn_baseline = False\n",
    "        self.reward_to_go = False\n",
    "        self.standardize_advantages = True\n",
    "        #self.loss_fn = loss_fn\n",
    "        #self.optimizer = optimizer\n",
    "        self.replay_buffer = ReplayBuffer(params['max_replay_buffer_size'])\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        return modelx(torch.Tensor(obs))\n",
    "        \n",
    "    def training_loop(self, epochs=10, log_video=False, relabel_with_expert=False):\n",
    "        logdir = \"./logs/demo_\" + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        writer = SummaryWriter(logdir)\n",
    "        total_envsteps = 0\n",
    "        for epoch in range(epochs):\n",
    "            #Collect trajectories, need model and env\n",
    "            paths, envsteps_this_batch = collect_training_trajectories(epoch, self.modelx, params['batch_size'])\n",
    "\n",
    "            total_envsteps += envsteps_this_batch\n",
    "\n",
    "            #Add paths to replay buffer\n",
    "            self.replay_buffer.add_rollouts(paths)\n",
    "\n",
    "            #Train agent\n",
    "            self.train_agent(epoch, writer)\n",
    "            \n",
    "        \n",
    "    def train_agent(self, epoch, writer):\n",
    "        #running_loss = 0.0\n",
    "        for train_step in range(params['num_agent_train_steps_per_iter']):\n",
    "            #sample from recent data\n",
    "            ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch = self.replay_buffer.sample_recent_data(\n",
    "                                                                            params['train_batch_size'], False)\n",
    "            \n",
    "            q_values = self.calculate_q_vals(re_batch)\n",
    "            advantage_values = self.estimate_advantage(ob_batch, q_values)\n",
    "            adv_n = torch.Tensor(advantage_values)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = self.modelx(torch.Tensor(ob_batch))\n",
    "            model_dist = torch.distributions.Categorical(logits=outputs)\n",
    "            #sam_ac = model_dist.sample()\n",
    "            logprob_n = model_dist.log_prob(torch.Tensor(ac_batch))\n",
    "            loss = (-logprob_n * adv_n).sum()\n",
    "            \n",
    "            #want to learn which tensor is backprop and which does not\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            #running_loss += loss.item()\n",
    "            \n",
    "            train_returns = np.mean([reward.sum() for reward in re_batch])\n",
    "\n",
    "            #if train_step % 1000 == 999:    # every 1000 mini-batches...\n",
    "                # ...log the running loss\n",
    "                #writer.add_scalar('training loss', running_loss / 1000, epoch )\n",
    "            writer.add_scalar('Average Rewards', train_returns, epoch )\n",
    "\n",
    "                #running_loss = 0.0\n",
    "\n",
    "    def calculate_q_vals(self, rews_list):\n",
    "        # Case 1: trajectory-based PG \n",
    "        if not self.reward_to_go:\n",
    "            q_values = np.concatenate([self._discounted_return(r) for r in rews_list])\n",
    "\n",
    "        # Case 2: reward-to-go PG \n",
    "        else:\n",
    "            q_values = np.concatenate([self._discounted_cumsum(r) for r in rews_list])\n",
    "\n",
    "        return q_values\n",
    "    \n",
    "    def estimate_advantage(self, obs, q_values):\n",
    "        # TODO: Estimate the advantage when nn_baseline is True\n",
    "        # HINT1: pass obs into the neural network that you're using to learn the baseline\n",
    "            # extra hint if you're stuck: see your actor's run_baseline_prediction\n",
    "        # HINT2: advantage should be [Q-b]\n",
    "        if self.nn_baseline:\n",
    "            b_n_unnormalized = self.actor.run_baseline_prediction(obs)\n",
    "            b_n = b_n_unnormalized * np.std(q_values) + np.mean(q_values)\n",
    "            adv_n = q_values - b_n\n",
    "\n",
    "        # Else, just set the advantage to [Q]\n",
    "        else:\n",
    "            adv_n = q_values.copy()\n",
    "\n",
    "        # Normalize the resulting advantages\n",
    "        if self.standardize_advantages:\n",
    "            adv_n = (adv_n - np.mean(adv_n)) / (np.std(adv_n) + 1e-8)\n",
    "\n",
    "        return adv_n\n",
    "    \n",
    "    def _discounted_return(self, rewards):\n",
    "        # 1) create a list of indices (t'): from 0 to T-1\n",
    "        indices = np.arange(len(rewards))\n",
    "\n",
    "        # 2) create a list where the entry at each index (t') is gamma^(t')\n",
    "        discounts = np.power(self.gamma, indices)\n",
    "\n",
    "        # 3) create a list where the entry at each index (t') is gamma^(t') * r_{t'}\n",
    "        discounted_rewards = discounts * np.array(rewards)\n",
    "\n",
    "        # 4) calculate a scalar: sum_{t'=0}^{T-1} gamma^(t') * r_{t'}\n",
    "        sum_of_discounted_rewards = np.sum(discounted_rewards)\n",
    "\n",
    "        # 5) create a list of length T-1, where each entry t contains that scalar\n",
    "        list_of_discounted_returns = sum_of_discounted_rewards * np.ones(indices.shape[0])\n",
    "\n",
    "        return list_of_discounted_returns\n",
    "    \n",
    "    def _discounted_cumsum(self, rewards):\n",
    "        all_discounted_cumsums = []\n",
    "\n",
    "        # for loop over steps (t) of the given rollout\n",
    "        for start_time_index in range(len(rewards)): \n",
    "\n",
    "            # 1) create a list of indices (t'): goes from t to T-1\n",
    "            indices = np.arange(len(rewards))\n",
    "\n",
    "            # 2) create a list where the entry at each index (t') is gamma^(t'-t)\n",
    "            discounts = np.power(self.gamma, indices - start_time_index)\n",
    "\n",
    "            # 3) create a list where the entry at each index (t') is gamma^(t'-t) * r_{t'}\n",
    "            # Hint: remember that t' goes from t to T-1, so you should use the rewards from those indices as well\n",
    "            discounted_rtg = np.array(rewards[start_time_index:]) * discounts[start_time_index:]\n",
    "\n",
    "            # 4) calculate a scalar: sum_{t'=t}^{T-1} gamma^(t'-t) * r_{t'}\n",
    "            sum_discounted_rtg = np.sum(discounted_rtg)\n",
    "\n",
    "            # appending each of these calculated sums into the list to return\n",
    "            all_discounted_cumsums.append(sum_discounted_rtg)\n",
    "        list_of_discounted_cumsums = np.array(all_discounted_cumsums)\n",
    "        return list_of_discounted_cumsums "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Path(obs, image_obs, acs, rewards, next_obs, terminals):\n",
    "    \"\"\"\n",
    "        Take info (separate arrays) from a single rollout\n",
    "        and return it in a single dictionary\n",
    "    \"\"\"\n",
    "    if image_obs != []:\n",
    "        image_obs = np.stack(image_obs, axis=0)\n",
    "    return {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "            \"image_obs\" : np.array(image_obs, dtype=np.uint8),\n",
    "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}\n",
    "\n",
    "def get_pathlength(path):\n",
    "    return len(path[\"reward\"])\n",
    "\n",
    "def sample_trajectory(env, modelx, max_path_length):\n",
    "\n",
    "    # initialize env for the beginning of a new rollout\n",
    "    ob = env.reset() # HINT: should be the output of resetting the env\n",
    "\n",
    "    # init varas\n",
    "    obs, acs, rewards, next_obs, terminals, image_obs = [], [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # use the most recent ob to decide what to do\n",
    "        obs.append(ob)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #ac = modelx(torch.Tensor([ob]))\n",
    "            output = modelx(torch.Tensor(ob))\n",
    "            m = torch.distributions.Categorical(logits=output)\n",
    "            ac = m.sample()\n",
    "            ac = ac.detach().numpy() # HINT: query the policy's get_action function TF object, change to PyTorch\n",
    "        #ac = ac[0]\n",
    "        acs.append(ac)\n",
    "\n",
    "        # take that action and record results\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "\n",
    "        # record result of taking that action\n",
    "        steps += 1\n",
    "        next_obs.append(ob)\n",
    "        rewards.append(rew)\n",
    "\n",
    "        # TODO end the rollout if the rollout ended \n",
    "        # HINT: rollout can end due to done, or due to max_path_length\n",
    "        rollout_done = (steps == max_path_length) or done # HINT: this is either 0 or 1\n",
    "        terminals.append(rollout_done)\n",
    "        \n",
    "        if rollout_done: \n",
    "            break\n",
    "\n",
    "    return Path(obs, image_obs, acs, rewards, next_obs, terminals)\n",
    "\n",
    "\n",
    "def sample_trajectories(env, policy, min_timesteps_per_batch, max_path_length):\n",
    "    \"\"\"\n",
    "        Collect rollouts until we have collected min_timesteps_per_batch steps.\n",
    "        TODO implement this function\n",
    "        Hint1: use sample_trajectory to get each path (i.e. rollout) that goes into paths\n",
    "        Hint2: use get_pathlength to count the timesteps collected in each path\n",
    "    \"\"\"\n",
    "    timesteps_this_batch = 0\n",
    "    paths = []\n",
    "    while timesteps_this_batch < min_timesteps_per_batch:\n",
    "        thispath = sample_trajectory(env, policy, max_path_length)\n",
    "        paths.append(thispath)\n",
    "        timesteps_this_batch += get_pathlength(thispath)\n",
    "\n",
    "    return paths, timesteps_this_batch\n",
    "\n",
    "\n",
    "\n",
    "def collect_training_trajectories(itr, model, batch_size):\n",
    "\n",
    "    #print(\"\\nCollecting data to be used for training...\")\n",
    "    paths, envsteps_this_batch = sample_trajectories(env, model, batch_size, params['ep_len'])\n",
    "\n",
    "    return paths, envsteps_this_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch = replay_buffer.sample_recent_data(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sam_obs = np.array([-0.03387007,  0.1474191 ,  0.03877906, -0.28876239])\n",
    "output = model(torch.Tensor(ob_batch))\n",
    "m = torch.distributions.Categorical(logits=output)\n",
    "sam_ac = m.sample()\n",
    "logprob_n = m.log_prob(torch.Tensor(ac_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprob_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_ac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = calculate_q_vals(re_batch)\n",
    "advantage_values = estimate_advantage(ob_batch, q_values)\n",
    "adv_n = torch.Tensor(advantage_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.9170, -6.0416, -6.1232, -6.0825, -5.8365, -5.5870, -5.3580, -5.1370,\n",
       "        -4.9167, -5.8745, -5.9961, -6.0797, -6.0588, -5.8322, -5.5854, -5.3609,\n",
       "        -5.1358, -4.9160, -5.8945, -6.0259, -6.1043, -6.0390, -5.8074, -5.5726,\n",
       "        -5.3471, -5.1177, -4.8906], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(logprob_n * adv_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6574, -0.6713, -0.6804, -0.6758, -0.6485, -0.6208, -0.5953, -0.5708,\n",
       "        -0.5463, -0.6527, -0.6662, -0.6755, -0.6732, -0.6480, -0.6206, -0.5957,\n",
       "        -0.5706, -0.5462, -0.6549, -0.6695, -0.6783, -0.6710, -0.6453, -0.6192,\n",
       "        -0.5941, -0.5686, -0.5434], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.log_prob(torch.Tensor(ac_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor([ob_batch[1]])).argmax().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "#want to learn which tensor is backprop and which does not\n",
    "#loss = loss_fn(logprob_n, torch.Tensor(advantage_values))\n",
    "\n",
    "#loss\n",
    "#env.reset()\n",
    "#next_state, reward, _, _ = env.step(sam_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 1, 0, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_q_vals(rews_list):\n",
    "    # Case 1: trajectory-based PG \n",
    "    q_values = np.concatenate([_discounted_return(r) for r in rews_list])\n",
    "\n",
    "    return q_values\n",
    "\n",
    "def estimate_advantage(obs, q_values):\n",
    "\n",
    "    adv_n = q_values.copy()\n",
    "\n",
    "    # Normalize the resulting advantages\n",
    "\n",
    "    #adv_n = (adv_n - np.mean(adv_n)) / (np.std(adv_n) + 1e-8)\n",
    "\n",
    "    return adv_n\n",
    "\n",
    "def _discounted_return(rewards):\n",
    "    gamma = 1.0\n",
    "    # 1) create a list of indices (t'): from 0 to T-1\n",
    "    indices = np.arange(len(rewards))\n",
    "\n",
    "    # 2) create a list where the entry at each index (t') is gamma^(t')\n",
    "    discounts = np.power(gamma, indices)\n",
    "\n",
    "    # 3) create a list where the entry at each index (t') is gamma^(t') * r_{t'}\n",
    "    discounted_rewards = discounts * np.array(rewards)\n",
    "\n",
    "    # 4) calculate a scalar: sum_{t'=0}^{T-1} gamma^(t') * r_{t'}\n",
    "    sum_of_discounted_rewards = np.sum(discounted_rewards)\n",
    "\n",
    "    # 5) create a list of length T-1, where each entry t contains that scalar\n",
    "    list_of_discounted_returns = sum_of_discounted_rewards * np.ones(indices.shape[0])\n",
    "\n",
    "    return list_of_discounted_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_paths, _ = collect_training_trajectories(1, model, params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sam_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.add_rollouts(sam_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.02140454,  0.0325623 ,  0.01619541,  0.04024277],\n",
       "        [-0.02075329, -0.16278811,  0.01700026,  0.33799118],\n",
       "        [-0.02400906,  0.03208786,  0.02376008,  0.05071731],\n",
       "        [-0.0233673 ,  0.2268612 ,  0.02477443, -0.23437543],\n",
       "        [-0.01883007,  0.03139419,  0.02008692,  0.06601804],\n",
       "        [-0.01820219, -0.1640099 ,  0.02140728,  0.3649702 ],\n",
       "        [-0.02148239,  0.03080138,  0.02870669,  0.07911351],\n",
       "        [-0.02086636,  0.2255003 ,  0.03028896, -0.20437594],\n",
       "        [-0.01635635,  0.02995859,  0.02620144,  0.09770569],\n",
       "        [-0.01575718, -0.1655289 ,  0.02815555,  0.39853862],\n",
       "        [-0.01906776,  0.02918255,  0.03612632,  0.11486384],\n",
       "        [-0.01848411,  0.22376874,  0.0384236 , -0.16620617],\n",
       "        [-0.01400874,  0.02811844,  0.03509948,  0.13834643],\n",
       "        [-0.01344637, -0.1674882 ,  0.03786641,  0.4418928 ],\n",
       "        [-0.01679613,  0.02707802,  0.04670426,  0.16138273],\n",
       "        [-0.01625457,  0.22150132,  0.04993192, -0.11620813],\n",
       "        [-0.01182454,  0.0257008 ,  0.04760775,  0.19180046],\n",
       "        [-0.01131053,  0.2201105 ,  0.05144376, -0.08549229],\n",
       "        [-0.00690832,  0.41445875,  0.04973392, -0.3615112 ],\n",
       "        [ 0.00138086,  0.21866646,  0.04250369, -0.05357015],\n",
       "        [ 0.00575419,  0.02296165,  0.04143229,  0.25221425],\n",
       "        [ 0.00621342, -0.17272668,  0.04647658,  0.55767244],\n",
       "        [ 0.00275889,  0.02171307,  0.05763002,  0.27998698],\n",
       "        [ 0.00319315,  0.21596761,  0.06322976,  0.00602276],\n",
       "        [ 0.0075125 ,  0.01999852,  0.06335022,  0.31796613],\n",
       "        [ 0.00791247,  0.21416366,  0.06970954,  0.04591516],\n",
       "        [ 0.01219574,  0.01811495,  0.07062785,  0.35975203],\n",
       "        [ 0.01255804,  0.21216556,  0.07782289,  0.09014912],\n",
       "        [ 0.01680135,  0.01601938,  0.07962587,  0.40633473],\n",
       "        [ 0.01712174,  0.20992725,  0.08775257,  0.13977979],\n",
       "        [ 0.02132029,  0.01366522,  0.09054816,  0.45880622],\n",
       "        [ 0.02159359,  0.20739833,  0.09972429,  0.19598186],\n",
       "        [ 0.02574156,  0.4009629 ,  0.10364392, -0.06365208],\n",
       "        [ 0.03376082,  0.5944579 ,  0.10237088, -0.32192034],\n",
       "        [ 0.04564997,  0.39803848,  0.09593247,  0.00121084],\n",
       "        [ 0.05361074,  0.20168096,  0.09595669,  0.32255375],\n",
       "        [ 0.05764436,  0.39531484,  0.10240776,  0.06160707],\n",
       "        [ 0.06555066,  0.58883077,  0.10363991, -0.19709076],\n",
       "        [ 0.07732727,  0.39239085,  0.09969809,  0.12640364],\n",
       "        [ 0.08517509,  0.19599247,  0.10222616,  0.44880104],\n",
       "        [ 0.08909494,  0.3895311 ,  0.11120219,  0.19001082],\n",
       "        [ 0.09688556,  0.58290106,  0.1150024 , -0.06562642],\n",
       "        [ 0.10854358,  0.38633415,  0.11368987,  0.2610125 ],\n",
       "        [ 0.11627027,  0.5796651 ,  0.11891012,  0.00624105],\n",
       "        [ 0.12786357,  0.3830563 ,  0.11903495,  0.33394897],\n",
       "        [ 0.13552469,  0.1864591 ,  0.12571393,  0.6616717 ],\n",
       "        [ 0.13925388,  0.37962842,  0.13894735,  0.41106698],\n",
       "        [ 0.14684644,  0.5725353 ,  0.1471687 ,  0.16521466],\n",
       "        [ 0.15829715,  0.7652776 ,  0.15047298, -0.077662  ],\n",
       "        [ 0.1736027 ,  0.5683548 ,  0.14891975,  0.25845683],\n",
       "        [ 0.1849698 ,  0.37145534,  0.15408888,  0.59415776],\n",
       "        [ 0.1923989 ,  0.56412303,  0.16597204,  0.35370347],\n",
       "        [ 0.20368136,  0.75654405,  0.17304611,  0.11761032],\n",
       "        [ 0.21881224,  0.5594195 ,  0.17539832,  0.45950297],\n",
       "        [ 0.23000064,  0.36230853,  0.18458837,  0.8019362 ],\n",
       "        [ 0.23724681,  0.55448437,  0.2006271 ,  0.5725308 ]],\n",
       "       dtype=float32),\n",
       " array([0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 1.], dtype=float32),\n",
       " [array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.], dtype=float32)],\n",
       " array([[-0.02075329, -0.16278811,  0.01700026,  0.33799118],\n",
       "        [-0.02400906,  0.03208786,  0.02376008,  0.05071731],\n",
       "        [-0.0233673 ,  0.2268612 ,  0.02477443, -0.23437543],\n",
       "        [-0.01883007,  0.03139419,  0.02008692,  0.06601804],\n",
       "        [-0.01820219, -0.1640099 ,  0.02140728,  0.3649702 ],\n",
       "        [-0.02148239,  0.03080138,  0.02870669,  0.07911351],\n",
       "        [-0.02086636,  0.2255003 ,  0.03028896, -0.20437594],\n",
       "        [-0.01635635,  0.02995859,  0.02620144,  0.09770569],\n",
       "        [-0.01575718, -0.1655289 ,  0.02815555,  0.39853862],\n",
       "        [-0.01906776,  0.02918255,  0.03612632,  0.11486384],\n",
       "        [-0.01848411,  0.22376874,  0.0384236 , -0.16620617],\n",
       "        [-0.01400874,  0.02811844,  0.03509948,  0.13834643],\n",
       "        [-0.01344637, -0.1674882 ,  0.03786641,  0.4418928 ],\n",
       "        [-0.01679613,  0.02707802,  0.04670426,  0.16138273],\n",
       "        [-0.01625457,  0.22150132,  0.04993192, -0.11620813],\n",
       "        [-0.01182454,  0.0257008 ,  0.04760775,  0.19180046],\n",
       "        [-0.01131053,  0.2201105 ,  0.05144376, -0.08549229],\n",
       "        [-0.00690832,  0.41445875,  0.04973392, -0.3615112 ],\n",
       "        [ 0.00138086,  0.21866646,  0.04250369, -0.05357015],\n",
       "        [ 0.00575419,  0.02296165,  0.04143229,  0.25221425],\n",
       "        [ 0.00621342, -0.17272668,  0.04647658,  0.55767244],\n",
       "        [ 0.00275889,  0.02171307,  0.05763002,  0.27998698],\n",
       "        [ 0.00319315,  0.21596761,  0.06322976,  0.00602276],\n",
       "        [ 0.0075125 ,  0.01999852,  0.06335022,  0.31796613],\n",
       "        [ 0.00791247,  0.21416366,  0.06970954,  0.04591516],\n",
       "        [ 0.01219574,  0.01811495,  0.07062785,  0.35975203],\n",
       "        [ 0.01255804,  0.21216556,  0.07782289,  0.09014912],\n",
       "        [ 0.01680135,  0.01601938,  0.07962587,  0.40633473],\n",
       "        [ 0.01712174,  0.20992725,  0.08775257,  0.13977979],\n",
       "        [ 0.02132029,  0.01366522,  0.09054816,  0.45880622],\n",
       "        [ 0.02159359,  0.20739833,  0.09972429,  0.19598186],\n",
       "        [ 0.02574156,  0.4009629 ,  0.10364392, -0.06365208],\n",
       "        [ 0.03376082,  0.5944579 ,  0.10237088, -0.32192034],\n",
       "        [ 0.04564997,  0.39803848,  0.09593247,  0.00121084],\n",
       "        [ 0.05361074,  0.20168096,  0.09595669,  0.32255375],\n",
       "        [ 0.05764436,  0.39531484,  0.10240776,  0.06160707],\n",
       "        [ 0.06555066,  0.58883077,  0.10363991, -0.19709076],\n",
       "        [ 0.07732727,  0.39239085,  0.09969809,  0.12640364],\n",
       "        [ 0.08517509,  0.19599247,  0.10222616,  0.44880104],\n",
       "        [ 0.08909494,  0.3895311 ,  0.11120219,  0.19001082],\n",
       "        [ 0.09688556,  0.58290106,  0.1150024 , -0.06562642],\n",
       "        [ 0.10854358,  0.38633415,  0.11368987,  0.2610125 ],\n",
       "        [ 0.11627027,  0.5796651 ,  0.11891012,  0.00624105],\n",
       "        [ 0.12786357,  0.3830563 ,  0.11903495,  0.33394897],\n",
       "        [ 0.13552469,  0.1864591 ,  0.12571393,  0.6616717 ],\n",
       "        [ 0.13925388,  0.37962842,  0.13894735,  0.41106698],\n",
       "        [ 0.14684644,  0.5725353 ,  0.1471687 ,  0.16521466],\n",
       "        [ 0.15829715,  0.7652776 ,  0.15047298, -0.077662  ],\n",
       "        [ 0.1736027 ,  0.5683548 ,  0.14891975,  0.25845683],\n",
       "        [ 0.1849698 ,  0.37145534,  0.15408888,  0.59415776],\n",
       "        [ 0.1923989 ,  0.56412303,  0.16597204,  0.35370347],\n",
       "        [ 0.20368136,  0.75654405,  0.17304611,  0.11761032],\n",
       "        [ 0.21881224,  0.5594195 ,  0.17539832,  0.45950297],\n",
       "        [ 0.23000064,  0.36230853,  0.18458837,  0.8019362 ],\n",
       "        [ 0.23724681,  0.55448437,  0.2006271 ,  0.5725308 ],\n",
       "        [ 0.2483365 ,  0.74631196,  0.21207772,  0.3491505 ]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1.], dtype=float32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.sample_recent_data(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(model, env, params, 0.9, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.training_loop(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

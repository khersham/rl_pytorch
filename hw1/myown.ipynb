{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext tensorboard\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# placeholder for parameter\n",
    "###########################\n",
    "\n",
    "logdir = \"./logs/demo4\"\n",
    "\n",
    "params = {\n",
    "    \"expert_policy_file\": \"cs285/policies/experts/Humanoid.pkl\",\n",
    "    \"expert_data\": \"cs285/expert_data/expert_data_Humanoid-v2.pkl\",\n",
    "    \"env_name\": \"Humanoid-v2\",\n",
    "    \"exp_name\": \"test_bc_humanoid\",\n",
    "    \"do_dagger\": False,\n",
    "    \"num_agent_train_steps_per_iter\": 1000,\n",
    "    \"n_iter\": 1,\n",
    "    \"batch_size\": 1000,\n",
    "    \"eval_batch_size\": 5000,\n",
    "    \"train_batch_size\":100,\n",
    "    \"n_layers\": 2,\n",
    "    \"size\": 64,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"video_log_freq\": 5,\n",
    "    \"scalar_log_freq\": 1,\n",
    "    \"use_gpu\": True,\n",
    "    \"which_gpu\":0,\n",
    "    \"max_replay_buffer_size\":1000000,\n",
    "    \"seed\":1,\n",
    "    \"logdir\":logdir\n",
    "}\n",
    "\n",
    "agent_params = {\n",
    "            'n_layers': params['n_layers'],\n",
    "            'size': params['size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'max_replay_buffer_size': params['max_replay_buffer_size'],\n",
    "            }\n",
    "\n",
    "params['agent_params'] = agent_params\n",
    "\n",
    "# params for saving rollout videos to tensorboard\n",
    "MAX_NVIDEO = 2\n",
    "MAX_VIDEO_LEN = 40\n",
    "\n",
    "log_video = True\n",
    "log_metric = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the gym environment\n",
    "seed = params['seed']\n",
    "env = gym.make(params['env_name'])\n",
    "env.seed(seed)\n",
    "\n",
    "# Maximum length for episodes\n",
    "params['ep_len'] = env.spec.max_episode_steps\n",
    "\n",
    "# Is this env continuous, or self.discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "params['agent_params']['discrete'] = discrete\n",
    "\n",
    "# Observation and action sizes\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "params['agent_params']['ac_dim'] = ac_dim\n",
    "params['agent_params']['ob_dim'] = ob_dim\n",
    "\n",
    "fps = env.env.metadata['video.frames_per_second']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(376,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Perhaps a deque is better\n",
    "##This function organise replay buffer for SARS\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, max_size=1000000):\n",
    "\n",
    "        self.max_size = max_size\n",
    "\n",
    "        # store each rollout\n",
    "        self.paths = []\n",
    "\n",
    "        # store (concatenated) component arrays from each rollout\n",
    "        self.obs = None\n",
    "        self.acs = None\n",
    "        self.rews = None\n",
    "        self.next_obs = None\n",
    "        self.terminals = None\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.obs:\n",
    "            return self.obs.shape[0]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def add_rollouts(self, paths, concat_rew=True):\n",
    "\n",
    "        # add new rollouts into our list of rollouts\n",
    "        for path in paths:\n",
    "            self.paths.append(path)\n",
    "\n",
    "        # convert new rollouts into their component arrays, and append them onto our arrays\n",
    "        observations, actions, rewards, next_observations, terminals = self.convert_listofrollouts(paths, concat_rew)\n",
    "\n",
    "        if self.obs is None:\n",
    "            self.obs = observations[-self.max_size:]\n",
    "            self.acs = actions[-self.max_size:]\n",
    "            self.rews = rewards[-self.max_size:]\n",
    "            self.next_obs = next_observations[-self.max_size:]\n",
    "            self.terminals = terminals[-self.max_size:]\n",
    "        else:\n",
    "            self.obs = np.concatenate([self.obs, observations])[-self.max_size:]\n",
    "            self.acs = np.concatenate([self.acs, actions])[-self.max_size:]\n",
    "            if concat_rew:\n",
    "                self.rews = np.concatenate([self.rews, rewards])[-self.max_size:]\n",
    "            else:\n",
    "                if isinstance(rewards, list):\n",
    "                    self.rews += rewards\n",
    "                else:\n",
    "                    self.rews.append(rewards)\n",
    "                self.rews = self.rews[-self.max_size:]\n",
    "            self.next_obs = np.concatenate([self.next_obs, next_observations])[-self.max_size:]\n",
    "            self.terminals = np.concatenate([self.terminals, terminals])[-self.max_size:]\n",
    "            \n",
    "    def convert_listofrollouts(self, paths, concat_rew=True):\n",
    "        \"\"\"\n",
    "            Take a list of rollout dictionaries\n",
    "            and return separate arrays,\n",
    "            where each array is a concatenation of that array from across the rollouts\n",
    "        \"\"\"\n",
    "        observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "        actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "        if concat_rew:\n",
    "            rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
    "        else:\n",
    "            rewards = [path[\"reward\"] for path in paths]\n",
    "        next_observations = np.concatenate([path[\"next_observation\"] for path in paths])\n",
    "        terminals = np.concatenate([path[\"terminal\"] for path in paths])\n",
    "        return observations, actions, rewards, next_observations, terminals\n",
    "\n",
    "    ########################################\n",
    "    ########################################\n",
    "\n",
    "    def sample_random_data(self, batch_size):\n",
    "        assert self.obs.shape[0] == self.acs.shape[0] == self.rews.shape[0] == self.next_obs.shape[0] == self.terminals.shape[0]\n",
    "\n",
    "        ## TODO return batch_size number of random entries from each of the 5 component arrays above\n",
    "        ## HINT 1: use np.random.permutation to sample random indices\n",
    "        ## HINT 2: return corresponding data points from each array (i.e., not different indices from each array)\n",
    "        ## HINT 3: look at the sample_recent_data function below\n",
    "        mask = np.random.permutation(self.obs.shape[0])\n",
    "        return self.obs[mask<batch_size], self.acs[mask<batch_size], self.rews[mask<batch_size], self.next_obs[mask<batch_size], self.terminals[mask<batch_size]\n",
    "\n",
    "    def sample_recent_data(self, batch_size=1):\n",
    "        return self.obs[-batch_size:], self.acs[-batch_size:], self.rews[-batch_size:], self.next_obs[-batch_size:], self.terminals[-batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(agent_params['max_replay_buffer_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs285/expert_data/expert_data_Humanoid-v2.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 376)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample Paths\n",
    "print(params['expert_data'])\n",
    "with open(params['expert_data'], \"rb\") as f:\n",
    "    loaded_paths = pickle.load(f)\n",
    "    \n",
    "replay_buffer.add_rollouts(loaded_paths)\n",
    "sam_obs, sam_act, sam_reward, sam_nextobs, sam_terminal = replay_buffer.sample_random_data(10)\n",
    "\n",
    "sam_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_dim = agent_params['ac_dim']\n",
    "ob_dim = agent_params['ob_dim']\n",
    "n_layers = agent_params['n_layers']\n",
    "size = agent_params['size']\n",
    "discrete = agent_params['discrete']\n",
    "learning_rate = agent_params['learning_rate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(ob_dim, size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(size, size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(size, ac_dim)\n",
    "    )\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, ob_dim, size, ac_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(ob_dim, size)\n",
    "        self.fc2 = torch.nn.Linear(size, size)\n",
    "        self.fc3 = torch.nn.Linear(size, ac_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = Net(ob_dim, size, ac_dim)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_logging(self, itr, paths, eval_policy, train_video_paths):\n",
    "\n",
    "    # collect eval trajectories, for logging\n",
    "    print(\"\\nCollecting data for eval...\")\n",
    "    eval_paths, eval_envsteps_this_batch = sample_trajectories(self.env, eval_policy, self.params['eval_batch_size'], self.params['ep_len'])\n",
    "\n",
    "    # save eval rollouts as videos in tensorboard event file\n",
    "    if self.log_video and train_video_paths != None:\n",
    "        print('\\nCollecting video rollouts eval')\n",
    "        eval_video_paths = sample_n_trajectories(self.env, eval_policy, MAX_NVIDEO, MAX_VIDEO_LEN, True)\n",
    "\n",
    "        #save train/eval videos\n",
    "        print('\\nSaving train rollouts as videos...')\n",
    "        self.logger.log_paths_as_videos(train_video_paths, itr, fps=self.fps, max_videos_to_save=MAX_NVIDEO,\n",
    "                                        video_title='train_rollouts')\n",
    "        self.logger.log_paths_as_videos(eval_video_paths, itr, fps=self.fps,max_videos_to_save=MAX_NVIDEO,\n",
    "                                         video_title='eval_rollouts')\n",
    "\n",
    "    # save eval metrics\n",
    "    if self.log_metrics:\n",
    "        # returns, for logging\n",
    "        train_returns = [path[\"reward\"].sum() for path in paths]\n",
    "        eval_returns = [eval_path[\"reward\"].sum() for eval_path in eval_paths]\n",
    "\n",
    "        # episode lengths, for logging\n",
    "        train_ep_lens = [len(path[\"reward\"]) for path in paths]\n",
    "        eval_ep_lens = [len(eval_path[\"reward\"]) for eval_path in eval_paths]\n",
    "\n",
    "        # decide what to log\n",
    "        logs = OrderedDict()\n",
    "        logs[\"Eval_AverageReturn\"] = np.mean(eval_returns)\n",
    "        logs[\"Eval_StdReturn\"] = np.std(eval_returns)\n",
    "        logs[\"Eval_MaxReturn\"] = np.max(eval_returns)\n",
    "        logs[\"Eval_MinReturn\"] = np.min(eval_returns)\n",
    "        logs[\"Eval_AverageEpLen\"] = np.mean(eval_ep_lens)\n",
    "\n",
    "        logs[\"Train_AverageReturn\"] = np.mean(train_returns)\n",
    "        logs[\"Train_StdReturn\"] = np.std(train_returns)\n",
    "        logs[\"Train_MaxReturn\"] = np.max(train_returns)\n",
    "        logs[\"Train_MinReturn\"] = np.min(train_returns)\n",
    "        logs[\"Train_AverageEpLen\"] = np.mean(train_ep_lens)\n",
    "\n",
    "        logs[\"Train_EnvstepsSoFar\"] = self.total_envsteps\n",
    "        logs[\"TimeSinceStart\"] = time.time() - self.start_time\n",
    "\n",
    "\n",
    "        if itr == 0:\n",
    "            self.initial_return = np.mean(train_returns)\n",
    "        logs[\"Initial_DataCollection_AverageReturn\"] = self.initial_return\n",
    "\n",
    "        # perform the logging\n",
    "        for key, value in logs.items():\n",
    "            print('{} : {}'.format(key, value))\n",
    "            self.logger.log_scalar(value, key, itr)\n",
    "        print('Done logging...\\n\\n')\n",
    "\n",
    "        self.logger.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(params['expert_policy_file'], \"rb\") as f:\n",
    "    expert_policy_file = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ExpertPolicy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-15a84bf68b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexpertpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExpertPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert_policy_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ExpertPolicy' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n"
     ]
    }
   ],
   "source": [
    "obsnorm_mean = expertpolicy.policy_params['obsnorm']['Standardizer']['mean_1_D']\n",
    "obsnorm_meansq = expertpolicy.policy_params['obsnorm']['Standardizer']['meansq_1_D']\n",
    "obsnorm_stdev = np.sqrt(np.maximum(0, obsnorm_meansq - np.square(obsnorm_mean)))\n",
    "print('obs', obsnorm_mean.shape, obsnorm_stdev.shape)\n",
    "\n",
    "curr_activations_bd = torch.Tensor((sam_obs[1].reshape(1,-1) - obsnorm_mean) / (obsnorm_stdev + 1e-6))\n",
    "layer_params = expertpolicy.policy_params['hidden']['FeedforwardNet']\n",
    "for layer_name in sorted(layer_params.keys()):\n",
    "    l = layer_params[layer_name]\n",
    "    W = torch.Tensor(l['AffineLayer']['W'])\n",
    "    b = torch.Tensor(l['AffineLayer']['b'])\n",
    "    curr_activations_bd = torch.tanh(torch.mm(curr_activations_bd, W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2668, -2.1136,  1.1157,  1.0326,  0.6725,  4.7755,  4.1224,  1.3461,\n",
       "          1.9001, -1.9799,  0.4929,  0.7351,  1.1593,  0.1483,  1.7030,  1.7201,\n",
       "         -0.5640]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expertpolicy.get_action(sam_obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Policy\n",
    "##################\n",
    "\n",
    "class BasePolicy:\n",
    "    def get_action(self, obs):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    \n",
    "\n",
    "class Policy(BasePolicy):\n",
    "    \n",
    "    def __init__(self, model, env, params, loss_fn, optimizer):\n",
    "        super().__init__()\n",
    "        #self.model = model\n",
    "        self.env = env\n",
    "        #self.loss_fn = loss_fn\n",
    "        #self.optimizer = optimizer\n",
    "        self.replay_buffer = ReplayBuffer(params['max_replay_buffer_size'])\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        return model(torch.Tensor(obs))\n",
    "        \n",
    "    def training_loop(self, epochs=10, log_video=False, relabel_with_expert=False):\n",
    "        \n",
    "        total_envsteps = 0\n",
    "        for epoch in range(epochs):\n",
    "            #Collect trajectories, need model and env\n",
    "            paths, envsteps_this_batch = collect_training_trajectories(epoch,\n",
    "                                            params['expert_data'], model,\n",
    "                                            params['batch_size'])\n",
    "\n",
    "            total_envsteps += envsteps_this_batch\n",
    "            \n",
    "            # relabel the collected obs with actions from a provided expert policy\n",
    "            if relabel_with_expert and epoch >= 1:\n",
    "                paths = self.do_relabel_with_expert(expert_policy, paths) \n",
    "\n",
    "            #Add paths to replay buffer\n",
    "            self.replay_buffer.add_rollouts(paths)\n",
    "\n",
    "            #Train agent\n",
    "            self.train_agent(epoch)\n",
    "            \n",
    "            #logging\n",
    "            if log_video:\n",
    "                perform_logging(epoch, paths, train_video_paths)\n",
    "            \n",
    "        \n",
    "    def train_agent(self, epoch):\n",
    "        running_loss = 0.0\n",
    "        for train_step in range(params['num_agent_train_steps_per_iter']):\n",
    "            ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch = self.replay_buffer.sample_random_data(\n",
    "                                                                            params['train_batch_size'])\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(torch.Tensor(ob_batch))\n",
    "            loss = loss_fn(outputs, torch.Tensor(ac_batch))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if train_step % 1000 == 999:    # every 1000 mini-batches...\n",
    "                # ...log the running loss\n",
    "                writer.add_scalar('training loss', running_loss / 1000, epoch * 1000 + train_step)\n",
    "\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    def do_relabel_with_expert(self, expert_policy, paths):\n",
    "        print(\"\\nRelabelling collected observations with labels from an expert policy...\")\n",
    "\n",
    "        # TODO relabel collected obsevations (from our policy) with labels from an expert policy\n",
    "        # HINT: query the policy (using the get_action function) with paths[i][\"observation\"]\n",
    "        # and replace paths[i][\"action\"] with these expert labels\n",
    "        for i in range(len(paths)):\n",
    "            paths[i][\"action\"] = expert_policy.get_action(paths[i][\"observation\"])\n",
    "\n",
    "        return paths\n",
    "    \n",
    "    \n",
    "class ExpertPolicy(BasePolicy):\n",
    "    def __init__(self, expert_policy_file):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nonlin_type = expert_policy_file['nonlin_type']\n",
    "        self.policy_params = expert_policy_file['GaussianPolicy']\n",
    "        assert set(self.policy_params.keys()) == {'logstdevs_1_Da', 'hidden', 'obsnorm', 'out'}\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        obsnorm_mean = self.policy_params['obsnorm']['Standardizer']['mean_1_D']\n",
    "        obsnorm_meansq = self.policy_params['obsnorm']['Standardizer']['meansq_1_D']\n",
    "        obsnorm_stdev = np.sqrt(np.maximum(0, obsnorm_meansq - np.square(obsnorm_mean)))\n",
    "\n",
    "        curr_activations_bd = torch.Tensor((obs - obsnorm_mean) / (obsnorm_stdev + 1e-6))\n",
    "        layer_params = self.policy_params['hidden']['FeedforwardNet']\n",
    "        for layer_name in sorted(layer_params.keys()):\n",
    "            l = layer_params[layer_name]\n",
    "            W = torch.Tensor(l['AffineLayer']['W'])\n",
    "            b = torch.Tensor(l['AffineLayer']['b'])\n",
    "            curr_activations_bd = torch.tanh(torch.mm(curr_activations_bd, W) + b)\n",
    "        \n",
    "        W = torch.Tensor(self.policy_params['out']['AffineLayer']['W'])\n",
    "        b = torch.Tensor(self.policy_params['out']['AffineLayer']['b'])\n",
    "        output_bo = torch.mm(curr_activations_bd, W) + b\n",
    "        \n",
    "        return output_bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "expertpolicy = ExpertPolicy(expert_policy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_act = model(torch.Tensor(sam_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_act.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, torch.Tensor(sam_obs))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(pred_act, torch.Tensor(sam_act))\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1686.8428, dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Path(obs, image_obs, acs, rewards, next_obs, terminals):\n",
    "    \"\"\"\n",
    "        Take info (separate arrays) from a single rollout\n",
    "        and return it in a single dictionary\n",
    "    \"\"\"\n",
    "    if image_obs != []:\n",
    "        image_obs = np.stack(image_obs, axis=0)\n",
    "    return {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "            \"image_obs\" : np.array(image_obs, dtype=np.uint8),\n",
    "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}\n",
    "\n",
    "def get_pathlength(path):\n",
    "    return len(path[\"reward\"])\n",
    "\n",
    "def sample_trajectory(env, model, max_path_length):\n",
    "\n",
    "    # initialize env for the beginning of a new rollout\n",
    "    ob = env.reset() # HINT: should be the output of resetting the env\n",
    "\n",
    "    # init varas\n",
    "    obs, acs, rewards, next_obs, terminals, image_obs = [], [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # use the most recent ob to decide what to do\n",
    "        obs.append(ob)\n",
    "        ac = model(torch.Tensor(ob)).detach().numpy() # HINT: query the policy's get_action function TF object, change to PyTorch\n",
    "        #ac = ac[0]\n",
    "        acs.append(ac)\n",
    "\n",
    "        # take that action and record results\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "\n",
    "        # record result of taking that action\n",
    "        steps += 1\n",
    "        next_obs.append(ob)\n",
    "        rewards.append(rew)\n",
    "\n",
    "        # TODO end the rollout if the rollout ended \n",
    "        # HINT: rollout can end due to done, or due to max_path_length\n",
    "        rollout_done = (steps == max_path_length) or done # HINT: this is either 0 or 1\n",
    "        terminals.append(rollout_done)\n",
    "        \n",
    "        if rollout_done: \n",
    "            break\n",
    "\n",
    "    return Path(obs, image_obs, acs, rewards, next_obs, terminals)\n",
    "\n",
    "\n",
    "def sample_trajectories(env, policy, min_timesteps_per_batch, max_path_length):\n",
    "    \"\"\"\n",
    "        Collect rollouts until we have collected min_timesteps_per_batch steps.\n",
    "        TODO implement this function\n",
    "        Hint1: use sample_trajectory to get each path (i.e. rollout) that goes into paths\n",
    "        Hint2: use get_pathlength to count the timesteps collected in each path\n",
    "    \"\"\"\n",
    "    timesteps_this_batch = 0\n",
    "    paths = []\n",
    "    while timesteps_this_batch < min_timesteps_per_batch:\n",
    "        thispath = sample_trajectory(env, policy, max_path_length)\n",
    "        paths.append(thispath)\n",
    "        timesteps_this_batch += get_pathlength(thispath)\n",
    "\n",
    "    return paths, timesteps_this_batch\n",
    "\n",
    "\n",
    "\n",
    "def collect_training_trajectories(itr, load_initial_expertdata, model, batch_size):\n",
    "    \"\"\"\n",
    "    :param itr:\n",
    "    :param load_initial_expertdata:  path to expert data pkl file\n",
    "    :param collect_policy:  the current policy using which we collect data\n",
    "    :param batch_size:  the number of transitions we collect\n",
    "    :return:\n",
    "        paths: a list trajectories\n",
    "        envsteps_this_batch: the sum over the numbers of environment steps in paths\n",
    "        train_video_paths: paths which also contain videos for visualization purposes\n",
    "    \"\"\"\n",
    "\n",
    "    if itr==0:\n",
    "        print(load_initial_expertdata)\n",
    "        with open(load_initial_expertdata, \"rb\") as f:\n",
    "            loaded_paths = pickle.load(f)\n",
    "        return loaded_paths, 0\n",
    "\n",
    "    print(\"\\nCollecting data to be used for training...\")\n",
    "    paths, envsteps_this_batch = sample_trajectories(env, model, batch_size, params['ep_len'])\n",
    "\n",
    "    return paths, envsteps_this_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting data to be used for training...\n"
     ]
    }
   ],
   "source": [
    "trajectories = collect_training_trajectories(1, params['expert_data'], model, params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 376)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories[0][1]['observation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(model, env, params, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs285/expert_data/expert_data_Humanoid-v2.pkl\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting data to be used for training...\n"
     ]
    }
   ],
   "source": [
    "policy.training_loop(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d3502264b730f00a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d3502264b730f00a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir $logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# placeholder for parameter\n",
    "###########################\n",
    "\n",
    "logdir = \"./logs/demo3\"\n",
    "\n",
    "params = {\n",
    "    \"expert_policy_file\": \"cs285/policies/experts/Humanoid.pkl\",\n",
    "    \"expert_data\": \"cs285/expert_data/expert_data_Humanoid-v2.pkl\",\n",
    "    \"env_name\": \"Humanoid-v2\",\n",
    "    \"exp_name\": \"test_bc_humanoid\",\n",
    "    \"do_dagger\": False,\n",
    "    \"num_agent_train_steps_per_iter\": 1000,\n",
    "    \"n_iter\": 1,\n",
    "    \"batch_size\": 1000,\n",
    "    \"eval_batch_size\": 5000,\n",
    "    \"train_batch_size\":100,\n",
    "    \"n_layers\": 2,\n",
    "    \"size\": 64,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"video_log_freq\": 5,\n",
    "    \"scalar_log_freq\": 1,\n",
    "    \"use_gpu\": True,\n",
    "    \"which_gpu\":0,\n",
    "    \"max_replay_buffer_size\":1000000,\n",
    "    \"seed\":1,\n",
    "    \"logdir\":logdir\n",
    "}\n",
    "\n",
    "agent_params = {\n",
    "            'n_layers': params['n_layers'],\n",
    "            'size': params['size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'max_replay_buffer_size': params['max_replay_buffer_size'],\n",
    "            }\n",
    "\n",
    "params['agent_params'] = agent_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the gym environment\n",
    "seed = params['seed']\n",
    "env = gym.make(params['env_name'])\n",
    "env.seed(seed)\n",
    "\n",
    "# Maximum length for episodes\n",
    "params['ep_len'] = env.spec.max_episode_steps\n",
    "\n",
    "# Is this env continuous, or self.discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "params['agent_params']['discrete'] = discrete\n",
    "\n",
    "# Observation and action sizes\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "params['agent_params']['ac_dim'] = ac_dim\n",
    "params['agent_params']['ob_dim'] = ob_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(376,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Perhaps a deque is better\n",
    "##This function organise replay buffer for SARS\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, max_size=1000000):\n",
    "\n",
    "        self.max_size = max_size\n",
    "\n",
    "        # store each rollout\n",
    "        self.paths = []\n",
    "\n",
    "        # store (concatenated) component arrays from each rollout\n",
    "        self.obs = None\n",
    "        self.acs = None\n",
    "        self.rews = None\n",
    "        self.next_obs = None\n",
    "        self.terminals = None\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.obs:\n",
    "            return self.obs.shape[0]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def add_rollouts(self, paths, concat_rew=True):\n",
    "\n",
    "        # add new rollouts into our list of rollouts\n",
    "        for path in paths:\n",
    "            self.paths.append(path)\n",
    "\n",
    "        # convert new rollouts into their component arrays, and append them onto our arrays\n",
    "        observations, actions, rewards, next_observations, terminals = self.convert_listofrollouts(paths, concat_rew)\n",
    "\n",
    "        if self.obs is None:\n",
    "            self.obs = observations[-self.max_size:]\n",
    "            self.acs = actions[-self.max_size:]\n",
    "            self.rews = rewards[-self.max_size:]\n",
    "            self.next_obs = next_observations[-self.max_size:]\n",
    "            self.terminals = terminals[-self.max_size:]\n",
    "        else:\n",
    "            self.obs = np.concatenate([self.obs, observations])[-self.max_size:]\n",
    "            self.acs = np.concatenate([self.acs, actions])[-self.max_size:]\n",
    "            if concat_rew:\n",
    "                self.rews = np.concatenate([self.rews, rewards])[-self.max_size:]\n",
    "            else:\n",
    "                if isinstance(rewards, list):\n",
    "                    self.rews += rewards\n",
    "                else:\n",
    "                    self.rews.append(rewards)\n",
    "                self.rews = self.rews[-self.max_size:]\n",
    "            self.next_obs = np.concatenate([self.next_obs, next_observations])[-self.max_size:]\n",
    "            self.terminals = np.concatenate([self.terminals, terminals])[-self.max_size:]\n",
    "            \n",
    "    def convert_listofrollouts(self, paths, concat_rew=True):\n",
    "        \"\"\"\n",
    "            Take a list of rollout dictionaries\n",
    "            and return separate arrays,\n",
    "            where each array is a concatenation of that array from across the rollouts\n",
    "        \"\"\"\n",
    "        observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "        actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "        if concat_rew:\n",
    "            rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
    "        else:\n",
    "            rewards = [path[\"reward\"] for path in paths]\n",
    "        next_observations = np.concatenate([path[\"next_observation\"] for path in paths])\n",
    "        terminals = np.concatenate([path[\"terminal\"] for path in paths])\n",
    "        return observations, actions, rewards, next_observations, terminals\n",
    "\n",
    "    ########################################\n",
    "    ########################################\n",
    "\n",
    "    def sample_random_data(self, batch_size):\n",
    "        assert self.obs.shape[0] == self.acs.shape[0] == self.rews.shape[0] == self.next_obs.shape[0] == self.terminals.shape[0]\n",
    "\n",
    "        ## TODO return batch_size number of random entries from each of the 5 component arrays above\n",
    "        ## HINT 1: use np.random.permutation to sample random indices\n",
    "        ## HINT 2: return corresponding data points from each array (i.e., not different indices from each array)\n",
    "        ## HINT 3: look at the sample_recent_data function below\n",
    "        mask = np.random.permutation(self.obs.shape[0])\n",
    "        return self.obs[mask<batch_size], self.acs[mask<batch_size], self.rews[mask<batch_size], self.next_obs[mask<batch_size], self.terminals[mask<batch_size]\n",
    "\n",
    "    def sample_recent_data(self, batch_size=1):\n",
    "        return self.obs[-batch_size:], self.acs[-batch_size:], self.rews[-batch_size:], self.next_obs[-batch_size:], self.terminals[-batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(agent_params['max_replay_buffer_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs285/expert_data/expert_data_Humanoid-v2.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 376)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample Paths\n",
    "print(params['expert_data'])\n",
    "with open(params['expert_data'], \"rb\") as f:\n",
    "    loaded_paths = pickle.load(f)\n",
    "    \n",
    "replay_buffer.add_rollouts(loaded_paths)\n",
    "sam_obs, sam_act, sam_reward, sam_nextobs, sam_terminal = replay_buffer.sample_random_data(10)\n",
    "\n",
    "sam_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_dim = agent_params['ac_dim']\n",
    "ob_dim = agent_params['ob_dim']\n",
    "n_layers = agent_params['n_layers']\n",
    "size = agent_params['size']\n",
    "discrete = agent_params['discrete']\n",
    "learning_rate = agent_params['learning_rate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(ob_dim, size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(size, size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(size, ac_dim)\n",
    "    )\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, ob_dim, size, ac_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(ob_dim, size)\n",
    "        self.fc2 = torch.nn.Linear(size, size)\n",
    "        self.fc3 = torch.nn.Linear(size, ac_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = Net(ob_dim, size, ac_dim)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Policy\n",
    "##################\n",
    "\n",
    "class Policy(object):\n",
    "    \n",
    "    def __init__(self, model, env, params, loss_fn, optimizer):\n",
    "        super(Policy, self).__init__()\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.replay_buffer = ReplayBuffer(params['max_replay_buffer_size'])\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        return self.model(torch.Tensor(obs))\n",
    "    \n",
    "    def update(self, obs, acs):\n",
    "        raise NotImplemntedError\n",
    "        \n",
    "    def training_loop(self, epochs=10):\n",
    "        running_loss = 0.0\n",
    "        total_envsteps = 0\n",
    "        for epoch in range(epochs):\n",
    "            #Collect trajectories, need model and env\n",
    "            paths, envsteps_this_batch = collect_training_trajectories(epoch,\n",
    "                                            params['expert_data'], model,\n",
    "                                            params['batch_size'])\n",
    "\n",
    "            total_envsteps += envsteps_this_batch\n",
    "\n",
    "            #Add paths to replay buffer\n",
    "            self.replay_buffer.add_rollouts(paths)\n",
    "\n",
    "            #Train agent\n",
    "            for train_step in range(params['num_agent_train_steps_per_iter']):\n",
    "                ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch = self.replay_buffer.sample_random_data(\n",
    "                                                                                params['train_batch_size'])\n",
    "\n",
    "                loss = self.train_agent(ob_batch, ac_batch)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if train_step % 1000 == 999:    # every 1000 mini-batches...\n",
    "                    # ...log the running loss\n",
    "                    writer.add_scalar('training loss', running_loss / 1000, epoch * 1000 + train_step)\n",
    "\n",
    "                    running_loss = 0.0\n",
    "            \n",
    "        \n",
    "    def train_agent(self, ob_batch, ac_batch):\n",
    "        # zero the parameter gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = self.model(torch.Tensor(ob_batch))\n",
    "        loss = self.loss_fn(outputs, torch.Tensor(ac_batch))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_act = model(torch.Tensor(sam_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 17])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_act.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, torch.Tensor(sam_obs))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(pred_act, torch.Tensor(sam_act))\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1686.8428, dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Path(obs, image_obs, acs, rewards, next_obs, terminals):\n",
    "    \"\"\"\n",
    "        Take info (separate arrays) from a single rollout\n",
    "        and return it in a single dictionary\n",
    "    \"\"\"\n",
    "    if image_obs != []:\n",
    "        image_obs = np.stack(image_obs, axis=0)\n",
    "    return {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "            \"image_obs\" : np.array(image_obs, dtype=np.uint8),\n",
    "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}\n",
    "\n",
    "def get_pathlength(path):\n",
    "    return len(path[\"reward\"])\n",
    "\n",
    "def sample_trajectory(env, model, max_path_length):\n",
    "\n",
    "    # initialize env for the beginning of a new rollout\n",
    "    ob = env.reset() # HINT: should be the output of resetting the env\n",
    "\n",
    "    # init varas\n",
    "    obs, acs, rewards, next_obs, terminals, image_obs = [], [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # use the most recent ob to decide what to do\n",
    "        obs.append(ob)\n",
    "        ac = model(torch.Tensor(ob)).detach().numpy() # HINT: query the policy's get_action function TF object, change to PyTorch\n",
    "        #ac = ac[0]\n",
    "        acs.append(ac)\n",
    "\n",
    "        # take that action and record results\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "\n",
    "        # record result of taking that action\n",
    "        steps += 1\n",
    "        next_obs.append(ob)\n",
    "        rewards.append(rew)\n",
    "\n",
    "        # TODO end the rollout if the rollout ended \n",
    "        # HINT: rollout can end due to done, or due to max_path_length\n",
    "        rollout_done = (steps == max_path_length) or done # HINT: this is either 0 or 1\n",
    "        terminals.append(rollout_done)\n",
    "        \n",
    "        if rollout_done: \n",
    "            break\n",
    "\n",
    "    return Path(obs, image_obs, acs, rewards, next_obs, terminals)\n",
    "\n",
    "\n",
    "def sample_trajectories(env, policy, min_timesteps_per_batch, max_path_length):\n",
    "    \"\"\"\n",
    "        Collect rollouts until we have collected min_timesteps_per_batch steps.\n",
    "        TODO implement this function\n",
    "        Hint1: use sample_trajectory to get each path (i.e. rollout) that goes into paths\n",
    "        Hint2: use get_pathlength to count the timesteps collected in each path\n",
    "    \"\"\"\n",
    "    timesteps_this_batch = 0\n",
    "    paths = []\n",
    "    while timesteps_this_batch < min_timesteps_per_batch:\n",
    "        thispath = sample_trajectory(env, policy, max_path_length)\n",
    "        paths.append(thispath)\n",
    "        timesteps_this_batch += get_pathlength(thispath)\n",
    "\n",
    "    return paths, timesteps_this_batch\n",
    "\n",
    "\n",
    "\n",
    "def collect_training_trajectories(itr, load_initial_expertdata, model, batch_size):\n",
    "    \"\"\"\n",
    "    :param itr:\n",
    "    :param load_initial_expertdata:  path to expert data pkl file\n",
    "    :param collect_policy:  the current policy using which we collect data\n",
    "    :param batch_size:  the number of transitions we collect\n",
    "    :return:\n",
    "        paths: a list trajectories\n",
    "        envsteps_this_batch: the sum over the numbers of environment steps in paths\n",
    "        train_video_paths: paths which also contain videos for visualization purposes\n",
    "    \"\"\"\n",
    "\n",
    "    if itr==0:\n",
    "        print(load_initial_expertdata)\n",
    "        with open(load_initial_expertdata, \"rb\") as f:\n",
    "            loaded_paths = pickle.load(f)\n",
    "        return loaded_paths, 0\n",
    "\n",
    "    print(\"\\nCollecting data to be used for training...\")\n",
    "    paths, envsteps_this_batch = sample_trajectories(env, model, batch_size, params['ep_len'])\n",
    "\n",
    "    return paths, envsteps_this_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting data to be used for training...\n"
     ]
    }
   ],
   "source": [
    "trajectories = collect_training_trajectories(1, params['expert_data'], model, params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 376)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories[0][1]['observation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(model, env, params, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs285/expert_data/expert_data_Humanoid-v2.pkl\n",
      "\n",
      "Collecting data to be used for training...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ep_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-d96c9a9f89ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-0a14e0b18415>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             paths, envsteps_this_batch = collect_training_trajectories(epoch,\n\u001b[1;32m     27\u001b[0m                                             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'expert_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                             params['batch_size'])\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtotal_envsteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menvsteps_this_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-f6014845fcdd>\u001b[0m in \u001b[0;36mcollect_training_trajectories\u001b[0;34m(itr, load_initial_expertdata, model, batch_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nCollecting data to be used for training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvsteps_this_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ep_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvsteps_this_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ep_len'"
     ]
    }
   ],
   "source": [
    "policy.training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-788b345638a3c972\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-788b345638a3c972\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6010;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir $logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 376)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "## AGENT\n",
    "#############\n",
    "\n",
    "class BaseAgent(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BaseAgent, self).__init__(**kwargs)\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def add_to_replay_buffer(self, paths):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "class BCAgent(BaseAgent):\n",
    "    def __init__(self, env, agent_params):\n",
    "        super(BCAgent, self).__init__()\n",
    "\n",
    "        # init vars\n",
    "        self.env = env\n",
    "        self.agent_params = agent_params\n",
    "\n",
    "        # actor/policy\n",
    "        self.actor = MLPPolicySL(\n",
    "                               self.agent_params['ac_dim'],\n",
    "                               self.agent_params['ob_dim'],\n",
    "                               self.agent_params['n_layers'],\n",
    "                               self.agent_params['size'],\n",
    "                               discrete = self.agent_params['discrete'],\n",
    "                               learning_rate = self.agent_params['learning_rate'],\n",
    "                               ) ## TODO: look in here and implement this\n",
    "\n",
    "        # replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(self.agent_params['max_replay_buffer_size'])\n",
    "\n",
    "    def train(self, ob_no, ac_na, re_n, next_ob_no, terminal_n):\n",
    "        # training a BC agent refers to updating its actor using\n",
    "        # the given observations and corresponding action labels\n",
    "        self.actor.update(ob_no, ac_na) ## TODO: look in here and implement this\n",
    "\n",
    "    def add_to_replay_buffer(self, paths):\n",
    "        self.replay_buffer.add_rollouts(paths)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return self.replay_buffer.sample_random_data(batch_size) ## TODO: look in here and implement this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kslim/Documents/virtualenv/drltf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kslim/Documents/virtualenv/drltf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kslim/Documents/virtualenv/drltf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kslim/Documents/virtualenv/drltf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kslim/Documents/virtualenv/drltf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kslim/Documents/virtualenv/drltf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gym\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "from tensorboardX import SummaryWriter\n",
    "import tensorflow.contrib.layers as layers\n",
    "from gym import wrappers\n",
    "from gym import spaces\n",
    "import cv2\n",
    "import random\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"./logs/demo_pong\"\n",
    "\n",
    "params = {\n",
    "    \"env_name\": \"PongNoFrameskip-v4\",\n",
    "    \"exp_name\": \"sb_no_rtg_dsa\",\n",
    "    \"num_agent_train_steps_per_iter\": 1000,\n",
    "    'num_critic_updates_per_agent_update': 1000,\n",
    "    'num_timesteps': int(2e8),\n",
    "    \"n_iter\": 10,\n",
    "    \"batch_size\": 1000,\n",
    "    \"eval_batch_size\": 500,\n",
    "    \"train_batch_size\":1000,\n",
    "    \"discount\": 1.0,\n",
    "    \"double_q\": False,\n",
    "    \"n_layers\": 2,\n",
    "    \"size\": 64,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"video_log_freq\": 5,\n",
    "    \"scalar_log_freq\": 1,\n",
    "    \"use_gpu\": True,\n",
    "    \"which_gpu\":0,\n",
    "    \"max_replay_buffer_size\":1000000,\n",
    "    \"seed\":1,\n",
    "    \"logdir\":logdir\n",
    "}\n",
    "\n",
    "agent_params = {'learning_starts': 50000,\n",
    "    'target_update_freq': 10000,\n",
    "    'replay_buffer_size': int(1e6),\n",
    "    'num_timesteps': int(2e8),\n",
    "    #'q_func': atari_model,\n",
    "    'learning_freq': 4,\n",
    "    'grad_norm_clipping': 10,\n",
    "    'input_shape': (84, 84, 4),\n",
    "    #'env_wrappers': wrap_deepmind,\n",
    "    'frame_history_len': 4,\n",
    "    'gamma': 0.99}\n",
    "\n",
    "params['agent_params'] = agent_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "seed = params['seed']\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make(params['env_name'])\n",
    "env = wrappers.Monitor(env, os.path.join(params['logdir'], \"gym\"), force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
    "            # so it's important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "    \n",
    "env = EpisodicLifeEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "    \n",
    "env = NoopResetEnv(env, noop_max=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why Maxpooling? \n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "env = MaxAndSkipEnv(env, skip=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 210, 160, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((2,)+env.observation_space.shape, dtype=np.uint8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "    \n",
    "if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "    env = FireResetEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_frame84(frame):\n",
    "    img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "    img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "    resized_screen = cv2.resize(img, (84, 110),  interpolation=cv2.INTER_LINEAR)\n",
    "    x_t = resized_screen[18:102, :]\n",
    "    x_t = np.reshape(x_t, [84, 84, 1])\n",
    "    return x_t.astype(np.uint8)\n",
    "\n",
    "class ProcessFrame84(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return _process_frame84(obs), reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return _process_frame84(self.env.reset())\n",
    "    \n",
    "    \n",
    "env = ProcessFrame84(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)\n",
    "    \n",
    "env = ClipRewardEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "mean_episode_reward = -float('nan')\n",
    "best_mean_episode_reward = -float('inf')\n",
    "\n",
    "# Maximum length for episodes\n",
    "params['ep_len'] = env.spec.max_episode_steps\n",
    "MAX_VIDEO_LEN = params['ep_len']\n",
    "\n",
    "# Is this env continuous, or self.discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "params['agent_params']['discrete'] = discrete\n",
    "img = len(env.observation_space.shape) > 2\n",
    "\n",
    "# Observation and action sizes\n",
    "ob_dim = env.observation_space.shape if img else env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "params['agent_params']['ac_dim'] = ac_dim\n",
    "params['agent_params']['ob_dim'] = ob_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_session(use_gpu, gpu_frac=0.6, allow_gpu_growth=True, which_gpu=0):\n",
    "    if use_gpu:\n",
    "        # gpu options\n",
    "        gpu_options = tf.GPUOptions(\n",
    "            per_process_gpu_memory_fraction=gpu_frac,\n",
    "            allow_growth=allow_gpu_growth)\n",
    "        # TF config\n",
    "        config = tf.ConfigProto(\n",
    "            gpu_options=gpu_options,\n",
    "            log_device_placement=False,\n",
    "            allow_soft_placement=True,\n",
    "            inter_op_parallelism_threads=1,\n",
    "            intra_op_parallelism_threads=1)\n",
    "        # set env variable to specify which gpu to use\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(which_gpu)\n",
    "    else:\n",
    "        # TF config without gpu\n",
    "        config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "\n",
    "    # use config to create TF session\n",
    "    sess = tf.Session(config=config)\n",
    "    return sess\n",
    "\n",
    "sess = create_tf_session(params['use_gpu'], which_gpu=params['which_gpu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(env, policy, max_path_length, render=False, render_mode=('rgb_array')):\n",
    "\n",
    "    # initialize env for the beginning of a new rollout\n",
    "    ob = env.reset() # HINT: should be the output of resetting the env\n",
    "\n",
    "    # init vars\n",
    "    obs, acs, rewards, next_obs, terminals, image_obs = [], [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "\n",
    "        # render image of the simulated env\n",
    "        if render:\n",
    "            if 'rgb_array' in render_mode:\n",
    "                if hasattr(env, 'sim'):\n",
    "                    image_obs.append(env.sim.render(camera_name='track', height=500, width=500)[::-1])\n",
    "                else:\n",
    "                    image_obs.append(env.render(mode=render_mode))\n",
    "            if 'human' in render_mode:\n",
    "                env.render(mode=render_mode)\n",
    "                time.sleep(env.model.opt.timestep)\n",
    "\n",
    "        # use the most recent ob to decide what to do\n",
    "        obs.append(ob)\n",
    "        ac = policy.get_action(ob) # HINT: query the policy's get_action function\n",
    "        ac = ac[0]\n",
    "        acs.append(ac)\n",
    "\n",
    "        # take that action and record results\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "\n",
    "        # record result of taking that action\n",
    "        steps += 1\n",
    "        next_obs.append(ob)\n",
    "        rewards.append(rew)\n",
    "\n",
    "        # TODO end the rollout if the rollout ended \n",
    "        # HINT: rollout can end due to done, or due to max_path_length\n",
    "        rollout_done = (steps == max_path_length) or done # HINT: this is either 0 or 1\n",
    "        terminals.append(rollout_done)\n",
    "        \n",
    "        if rollout_done: \n",
    "            break\n",
    "\n",
    "    return Path(obs, image_obs, acs, rewards, next_obs, terminals)\n",
    "\n",
    "def sample_trajectories(env, policy, min_timesteps_per_batch, max_path_length, render=False, render_mode=('rgb_array')):\n",
    "    \"\"\"\n",
    "        Collect rollouts until we have collected min_timesteps_per_batch steps.\n",
    "\n",
    "        TODO implement this function\n",
    "        Hint1: use sample_trajectory to get each path (i.e. rollout) that goes into paths\n",
    "        Hint2: use get_pathlength to count the timesteps collected in each path\n",
    "    \"\"\"\n",
    "    timesteps_this_batch = 0\n",
    "    paths = []\n",
    "    while timesteps_this_batch < min_timesteps_per_batch:\n",
    "        thispath = sample_trajectory(env, policy, max_path_length, render, render_mode)\n",
    "        paths.append(thispath)\n",
    "        timesteps_this_batch += get_pathlength(thispath) \n",
    "\n",
    "    return paths, timesteps_this_batch\n",
    "\n",
    "def sample_n_trajectories(env, policy, ntraj, max_path_length, render=False, render_mode=('rgb_array')):\n",
    "    \"\"\"\n",
    "        Collect ntraj rollouts.\n",
    "\n",
    "        TODO implement this function\n",
    "        Hint1: use sample_trajectory to get each path (i.e. rollout) that goes into paths\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "\n",
    "    for _ in range(ntraj):\n",
    "        paths.append(sample_trajectory(env, policy, max_path_length, render, render_mode))\n",
    "\n",
    "    return paths\n",
    "\n",
    "############################################\n",
    "############################################\n",
    "\n",
    "def Path(obs, image_obs, acs, rewards, next_obs, terminals):\n",
    "    \"\"\"\n",
    "        Take info (separate arrays) from a single rollout\n",
    "        and return it in a single dictionary\n",
    "    \"\"\"\n",
    "    if image_obs != []:\n",
    "        image_obs = np.stack(image_obs, axis=0)\n",
    "    return {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "            \"image_obs\" : np.array(image_obs, dtype=np.uint8),\n",
    "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}\n",
    "\n",
    "\n",
    "def convert_listofrollouts(paths):\n",
    "    \"\"\"\n",
    "        Take a list of rollout dictionaries\n",
    "        and return separate arrays,\n",
    "        where each array is a concatenation of that array from across the rollouts\n",
    "    \"\"\"\n",
    "    observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "    actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "    next_observations = np.concatenate([path[\"next_observation\"] for path in paths])\n",
    "    terminals = np.concatenate([path[\"terminal\"] for path in paths])\n",
    "    concatenated_rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
    "    unconcatenated_rewards = [path[\"reward\"] for path in paths]\n",
    "    return observations, actions, next_observations, terminals, concatenated_rewards, unconcatenated_rewards\n",
    "\n",
    "############################################\n",
    "############################################\n",
    "\n",
    "def get_pathlength(path):\n",
    "    return len(path[\"reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BaseAgent, self).__init__(**kwargs)\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_n_unique(sampling_f, n):\n",
    "    \"\"\"Helper function. Given a function `sampling_f` that returns\n",
    "    comparable objects, sample n such unique objects.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    while len(res) < n:\n",
    "        candidate = sampling_f()\n",
    "        if candidate not in res:\n",
    "            res.append(candidate)\n",
    "    return res\n",
    "\n",
    "#This gives you 4framed observation\n",
    "class MemoryOptimizedReplayBuffer(object):\n",
    "    def __init__(self, size, frame_history_len, lander=False):\n",
    "        \"\"\"This is a memory efficient implementation of the replay buffer.\n",
    "\n",
    "        The sepecific memory optimizations use here are:\n",
    "            - only store each frame once rather than k times\n",
    "              even if every observation normally consists of k last frames\n",
    "            - store frames as np.uint8 (actually it is most time-performance\n",
    "              to cast them back to float32 on GPU to minimize memory transfer\n",
    "              time)\n",
    "            - store frame_t and frame_(t+1) in the same buffer.\n",
    "\n",
    "        For the tipical use case in Atari Deep RL buffer with 1M frames the total\n",
    "        memory footprint of this buffer is 10^6 * 84 * 84 bytes ~= 7 gigabytes\n",
    "\n",
    "        Warning! Assumes that returning frame of zeros at the beginning\n",
    "        of the episode, when there is less frames than `frame_history_len`,\n",
    "        is acceptable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        frame_history_len: int\n",
    "            Number of memories to be retried for each observation.\n",
    "        \"\"\"\n",
    "        self.lander = lander\n",
    "\n",
    "        self.size = size\n",
    "        self.frame_history_len = frame_history_len\n",
    "\n",
    "        self.next_idx      = 0\n",
    "        self.num_in_buffer = 0\n",
    "\n",
    "        self.obs      = None\n",
    "        self.action   = None\n",
    "        self.reward   = None\n",
    "        self.done     = None\n",
    "\n",
    "    def can_sample(self, batch_size):\n",
    "        \"\"\"Returns true if `batch_size` different transitions can be sampled from the buffer.\"\"\"\n",
    "        return batch_size + 1 <= self.num_in_buffer\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obs_batch      = np.concatenate([self._encode_observation(idx)[None] for idx in idxes], 0)\n",
    "        act_batch      = self.action[idxes]\n",
    "        rew_batch      = self.reward[idxes]\n",
    "        next_obs_batch = np.concatenate([self._encode_observation(idx + 1)[None] for idx in idxes], 0)\n",
    "        done_mask      = np.array([1.0 if self.done[idx] else 0.0 for idx in idxes], dtype=np.float32)\n",
    "\n",
    "        return obs_batch, act_batch, rew_batch, next_obs_batch, done_mask\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample `batch_size` different transitions.\n",
    "\n",
    "        i-th sample transition is the following:\n",
    "\n",
    "        when observing `obs_batch[i]`, action `act_batch[i]` was taken,\n",
    "        after which reward `rew_batch[i]` was received and subsequent\n",
    "        observation  next_obs_batch[i] was observed, unless the epsiode\n",
    "        was done which is represented by `done_mask[i]` which is equal\n",
    "        to 1 if episode has ended as a result of that action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            Array of shape\n",
    "            (batch_size, img_h, img_w, img_c * frame_history_len)\n",
    "            and dtype np.uint8\n",
    "        act_batch: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "        rew_batch: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "        next_obs_batch: np.array\n",
    "            Array of shape\n",
    "            (batch_size, img_h, img_w, img_c * frame_history_len)\n",
    "            and dtype np.uint8\n",
    "        done_mask: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "        \"\"\"\n",
    "        assert self.can_sample(batch_size)\n",
    "        idxes = sample_n_unique(lambda: random.randint(0, self.num_in_buffer - 2), batch_size)\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "    def encode_recent_observation(self):\n",
    "        \"\"\"Return the most recent `frame_history_len` frames.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        observation: np.array\n",
    "            Array of shape (img_h, img_w, img_c * frame_history_len)\n",
    "            and dtype np.uint8, where observation[:, :, i*img_c:(i+1)*img_c]\n",
    "            encodes frame at time `t - frame_history_len + i`\n",
    "        \"\"\"\n",
    "        assert self.num_in_buffer > 0\n",
    "        return self._encode_observation((self.next_idx - 1) % self.size)\n",
    "\n",
    "    def _encode_observation(self, idx):\n",
    "        end_idx   = idx + 1 # make noninclusive\n",
    "        start_idx = end_idx - self.frame_history_len\n",
    "        # this checks if we are using low-dimensional observations, such as RAM\n",
    "        # state, in which case we just directly return the latest RAM.\n",
    "        if len(self.obs.shape) == 2:\n",
    "            return self.obs[end_idx-1]\n",
    "        # if there weren't enough frames ever in the buffer for context\n",
    "        if start_idx < 0 and self.num_in_buffer != self.size:\n",
    "            start_idx = 0\n",
    "        for idx in range(start_idx, end_idx - 1):\n",
    "            if self.done[idx % self.size]:\n",
    "                start_idx = idx + 1\n",
    "        missing_context = self.frame_history_len - (end_idx - start_idx)\n",
    "        # if zero padding is needed for missing context\n",
    "        # or we are on the boundry of the buffer\n",
    "        if start_idx < 0 or missing_context > 0:\n",
    "            frames = [np.zeros_like(self.obs[0]) for _ in range(missing_context)]\n",
    "            for idx in range(start_idx, end_idx):\n",
    "                frames.append(self.obs[idx % self.size])\n",
    "            return np.concatenate(frames, 2)\n",
    "        else:\n",
    "            # this optimization has potential to saves about 30% compute time \\o/\n",
    "            img_h, img_w = self.obs.shape[1], self.obs.shape[2]\n",
    "            return self.obs[start_idx:end_idx].transpose(1, 2, 0, 3).reshape(img_h, img_w, -1)\n",
    "\n",
    "    def store_frame(self, frame):\n",
    "        \"\"\"Store a single frame in the buffer at the next available index, overwriting\n",
    "        old frames if necessary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        frame: np.array\n",
    "            Array of shape (img_h, img_w, img_c) and dtype np.uint8\n",
    "            the frame to be stored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            Index at which the frame is stored. To be used for `store_effect` later.\n",
    "        \"\"\"\n",
    "        if self.obs is None:\n",
    "            self.obs      = np.empty([self.size] + list(frame.shape), dtype=np.float32 if self.lander else np.uint8)\n",
    "            self.action   = np.empty([self.size],                     dtype=np.int32)\n",
    "            self.reward   = np.empty([self.size],                     dtype=np.float32)\n",
    "            self.done     = np.empty([self.size],                     dtype=np.bool)\n",
    "        self.obs[self.next_idx] = frame\n",
    "\n",
    "        ret = self.next_idx\n",
    "        self.next_idx = (self.next_idx + 1) % self.size\n",
    "        self.num_in_buffer = min(self.size, self.num_in_buffer + 1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def store_effect(self, idx, action, reward, done):\n",
    "        \"\"\"Store effects of action taken after obeserving frame stored\n",
    "        at index idx. The reason `store_frame` and `store_effect` is broken\n",
    "        up into two functions is so that once can call `encode_recent_observation`\n",
    "        in between.\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        idx: int\n",
    "            Index in buffer of recently observed frame (returned by `store_frame`).\n",
    "        action: int\n",
    "            Action that was performed upon observing this frame.\n",
    "        reward: float\n",
    "            Reward that was received when the actions was performed.\n",
    "        done: bool\n",
    "            True if episode was finished after performing that action.\n",
    "        \"\"\"\n",
    "        self.action[idx] = action\n",
    "        self.reward[idx] = reward\n",
    "        self.done[idx]   = done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = MemoryOptimizedReplayBuffer(agent_params['replay_buffer_size'], \n",
    "                                            agent_params['frame_history_len'], lander=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    obs,_ , done, _ = env.step(random.choice([2,3]))\n",
    "    myint = replay_buffer.store_frame(obs)\n",
    "    #print(myint)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = replay_buffer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 84, 84, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.encode_recent_observation()[None,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCritic(object):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def update(self, ob_no, next_ob_no, re_n, terminal_n):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "def huber_loss(x, delta=1.0):\n",
    "    # https://en.wikipedia.org/wiki/Huber_loss\n",
    "    return tf.where(\n",
    "        tf.abs(x) < delta,\n",
    "        tf.square(x) * 0.5,\n",
    "        delta * (tf.abs(x) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "def minimize_and_clip(optimizer, objective, var_list, clip_val=10):\n",
    "    \"\"\"Minimized `objective` using `optimizer` w.r.t. variables in\n",
    "    `var_list` while ensure the norm of the gradients for each\n",
    "    variable is clipped to `clip_val`\n",
    "    \"\"\"\n",
    "    gradients = optimizer.compute_gradients(objective, var_list=var_list)\n",
    "    for i, (grad, var) in enumerate(gradients):\n",
    "        if grad is not None:\n",
    "            gradients[i] = (tf.clip_by_norm(grad, clip_val), var)\n",
    "    return optimizer.apply_gradients(gradients)\n",
    "\n",
    "def atari_model(img_input, num_actions, scope, reuse=False):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = tf.cast(img_input, tf.float32) / 255.0\n",
    "        with tf.variable_scope(\"convnet\"):\n",
    "            # original architecture\n",
    "            out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n",
    "        out = layers.flatten(out)\n",
    "        with tf.variable_scope(\"action_value\"):\n",
    "            out = layers.fully_connected(out, num_outputs=512, activation_fn=tf.nn.relu)\n",
    "            out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "        return out\n",
    "    \n",
    "def linear_interpolation(l, r, alpha):\n",
    "    return l + alpha * (r - l)\n",
    "    \n",
    "class PiecewiseSchedule(object):\n",
    "    def __init__(self, endpoints, interpolation=linear_interpolation, outside_value=None):\n",
    "        \"\"\"Piecewise schedule.\n",
    "        endpoints: [(int, int)]\n",
    "            list of pairs `(time, value)` meanining that schedule should output\n",
    "            `value` when `t==time`. All the values for time must be sorted in\n",
    "            an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n",
    "            and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n",
    "            `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n",
    "            time passed between `time_a` and `time_b` for time `t`.\n",
    "        interpolation: lambda float, float, float: float\n",
    "            a function that takes value to the left and to the right of t according\n",
    "            to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n",
    "            right endpoint that t has covered. See linear_interpolation for example.\n",
    "        outside_value: float\n",
    "            if the value is requested outside of all the intervals sepecified in\n",
    "            `endpoints` this value is returned. If None then AssertionError is\n",
    "            raised when outside value is requested.\n",
    "        \"\"\"\n",
    "        idxes = [e[0] for e in endpoints]\n",
    "        assert idxes == sorted(idxes)\n",
    "        self._interpolation = interpolation\n",
    "        self._outside_value = outside_value\n",
    "        self._endpoints      = endpoints\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        for (l_t, l), (r_t, r) in zip(self._endpoints[:-1], self._endpoints[1:]):\n",
    "            if l_t <= t and t < r_t:\n",
    "                alpha = float(t - l_t) / (r_t - l_t)\n",
    "                return self._interpolation(l, r, alpha)\n",
    "\n",
    "        # t does not belong to any of the pieces, so doom.\n",
    "        assert self._outside_value is not None\n",
    "        return self._outside_value\n",
    "    \n",
    "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\", \"lr_schedule\"])\n",
    "    \n",
    "def atari_optimizer(num_timesteps):\n",
    "    num_iterations = num_timesteps/4\n",
    "    lr_multiplier = 1.0\n",
    "    lr_schedule = PiecewiseSchedule([\n",
    "        (0, 1e-4 * lr_multiplier),\n",
    "        (num_iterations / 10, 1e-4 * lr_multiplier),\n",
    "        (num_iterations / 2, 5e-5 * lr_multiplier),\n",
    "    ],\n",
    "        outside_value=5e-5 * lr_multiplier)\n",
    "\n",
    "    return OptimizerSpec(\n",
    "        constructor=tf.train.AdamOptimizer,\n",
    "        kwargs=dict(epsilon=1e-4),\n",
    "        lr_schedule=lr_schedule\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNCritic(BaseCritic):\n",
    "\n",
    "    def __init__(self, sess, hparams, optimizer_spec, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sess = sess\n",
    "        self.env_name = hparams['env_name']\n",
    "        self.ob_dim = hparams[\"agent_params\"]['ob_dim']\n",
    "\n",
    "        if isinstance(self.ob_dim, int):\n",
    "            self.input_shape = (self.ob_dim,)\n",
    "        else:\n",
    "            self.input_shape = hparams[\"agent_params\"]['input_shape']\n",
    "\n",
    "        self.ac_dim = hparams[\"agent_params\"]['ac_dim']\n",
    "        self.double_q = False\n",
    "        self.grad_norm_clipping = hparams[\"agent_params\"]['grad_norm_clipping']\n",
    "        self.gamma = hparams[\"agent_params\"]['gamma']\n",
    "\n",
    "        self.optimizer_spec = optimizer_spec\n",
    "        self.define_placeholders()\n",
    "        self._build(atari_model)\n",
    "\n",
    "    def _build(self, q_func):\n",
    "\n",
    "        #####################\n",
    "\n",
    "        # q values, created with the placeholder that holds CURRENT obs (i.e., t)\n",
    "        self.q_t_values = q_func(self.obs_t_ph, self.ac_dim, scope='q_func', reuse=False)\n",
    "        self.q_t = tf.reduce_sum(self.q_t_values * tf.one_hot(self.act_t_ph, self.ac_dim), axis=1)\n",
    "\n",
    "        #####################\n",
    "        #\n",
    "        # Q_phi - (r + gamma*max_a Q_phi')\n",
    "        #\n",
    "        #####################\n",
    "\n",
    "        # target q values, created with the placeholder that holds NEXT obs (i.e., t+1)\n",
    "        q_tp1_values = q_func(self.obs_tp1_ph, self.ac_dim, scope='target_q_func', reuse=False)\n",
    "\n",
    "        if self.double_q:\n",
    "            # You must fill this part for Q2 of the Q-learning potion of the homework.\n",
    "            # In double Q-learning, the best action is selected using the Q-network that\n",
    "            # is being updated, but the Q-value for this action is obtained from the\n",
    "            # target Q-network. See page 5 of https://arxiv.org/pdf/1509.06461.pdf for more details.\n",
    "            self.q_t_for_tp1 = q_func(self.obs_tp1_ph, self.ac_dim, scope='q_func', reuse=True)\n",
    "            idx = tf.stack([tf.range(tf.shape(self.obs_t_ph)[0]), \n",
    "                            tf.cast(tf.argmax(self.q_t_for_tp1, axis=1), tf.int32)], axis=1)\n",
    "            q_tp1 = tf.gather_nd(q_tp1_values, idx)\n",
    "        else:\n",
    "            # q values of the next timestep\n",
    "            q_tp1 = tf.reduce_max(q_tp1_values, axis=1)\n",
    "\n",
    "        #####################\n",
    "\n",
    "        # TODO calculate the targets for the Bellman error\n",
    "        # HINT1: as you saw in lecture, this would be:\n",
    "            #currentReward + self.gamma * qValuesOfNextTimestep * (1 - self.done_mask_ph)\n",
    "        # HINT2: see above, where q_tp1 is defined as the q values of the next timestep\n",
    "        # HINT3: see the defined placeholders and look for the one that holds current rewards\n",
    "        target_q_t = self.rew_t_ph + self.gamma * q_tp1 * (1-self.done_mask_ph)\n",
    "        target_q_t = tf.stop_gradient(target_q_t)\n",
    "\n",
    "        #####################\n",
    "\n",
    "        # TODO compute the Bellman error (i.e. TD error between q_t and target_q_t)\n",
    "        # Note that this scalar-valued tensor later gets passed into the optimizer, to be minimized\n",
    "        # HINT: use reduce mean of huber_loss (from infrastructure/dqn_utils.py) instead of squared error\n",
    "        self.total_error= tf.reduce_mean(huber_loss(target_q_t - self.q_t))\n",
    "\n",
    "        #####################\n",
    "\n",
    "        # TODO these variables should all of the \n",
    "        # variables of the Q-function network and target network, respectively\n",
    "        # HINT1: see the \"scope\" under which the variables were constructed in the lines at the top of this function\n",
    "        # HINT2: use tf.get_collection to look for all variables under a certain scope\n",
    "        q_func_vars = tf.trainable_variables(scope='q_func')\n",
    "        target_q_func_vars = tf.trainable_variables(scope='target_q_func')\n",
    "\n",
    "        #####################\n",
    "\n",
    "        # train_fn will be called in order to train the critic (by minimizing the TD error)\n",
    "        self.learning_rate = tf.placeholder(tf.float32, (), name=\"learning_rate\")\n",
    "        optimizer = self.optimizer_spec.constructor(learning_rate=self.learning_rate, **self.optimizer_spec.kwargs)\n",
    "        self.train_fn = minimize_and_clip(optimizer, self.total_error,\n",
    "                                          var_list=q_func_vars, clip_val=self.grad_norm_clipping)\n",
    "\n",
    "        # update_target_fn will be called periodically to copy Q network to target Q network\n",
    "        update_target_fn = []\n",
    "        for var, var_target in zip(sorted(q_func_vars,        key=lambda v: v.name),\n",
    "                                   sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "            update_target_fn.append(var_target.assign(var))\n",
    "        self.update_target_fn = tf.group(*update_target_fn)\n",
    "\n",
    "    def define_placeholders(self):\n",
    "        # set up placeholders\n",
    "        # placeholder for current observation (or state)\n",
    "        lander = self.env_name == 'LunarLander-v2'\n",
    "\n",
    "        self.obs_t_ph = tf.placeholder(\n",
    "            tf.float32 if lander else tf.uint8, [None] + list(self.input_shape))\n",
    "        # placeholder for current action\n",
    "        self.act_t_ph = tf.placeholder(tf.int32, [None])\n",
    "        # placeholder for current reward\n",
    "        self.rew_t_ph = tf.placeholder(tf.float32, [None])\n",
    "        # placeholder for next observation (or state)\n",
    "        self.obs_tp1_ph = tf.placeholder(\n",
    "            tf.float32 if lander else tf.uint8, [None] + list(self.input_shape))\n",
    "        # placeholder for end of episode mask\n",
    "        # this value is 1 if the next state corresponds to the end of an episode,\n",
    "        # in which case there is no Q-value at the next state; at the end of an\n",
    "        # episode, only the current state reward contributes to the target, not the\n",
    "        # next state Q-value (i.e. target is just rew_t_ph, not rew_t_ph + gamma * q_tp1)\n",
    "        self.done_mask_ph = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "    def update(self, ob_no, next_ob_no, re_n, terminal_n):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = DQNCritic(sess, params, atari_optimizer(params['num_timesteps']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 84, 84, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_ob_no.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'q_func_1/action_value/fully_connected_1/BiasAdd:0' shape=(100, 6) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atari_model(ob_no, 6, \"q_func\", reuse=tf.AUTO_REUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_no, ac_na, re_n, next_ob_no, terminal_n = replay_buffer.sample(100)\n",
    "\n",
    "feed_dict = {\n",
    "                critic.learning_rate: atari_optimizer(params['num_timesteps']).lr_schedule.value(0),\n",
    "                critic.obs_t_ph: ob_no,\n",
    "                critic.act_t_ph: ac_na,\n",
    "                critic.rew_t_ph: re_n,\n",
    "                critic.obs_tp1_ph: next_ob_no,\n",
    "                critic.done_mask_ph: terminal_n,\n",
    "            }\n",
    "\n",
    "sess.run(critic.train_fn, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ArgMaxPolicy(object):\n",
    "\n",
    "    def __init__(self, sess, critic):\n",
    "        self.sess = sess\n",
    "        self.critic = critic\n",
    "\n",
    "        # TODO: Define what action this policy should return\n",
    "        # HINT1: the critic's q_t_values indicate the goodness of observations, \n",
    "        # so they should be used to decide the action to perform\n",
    "        self.action = tf.argmax(self.critic.q_t_values, axis=1)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "\n",
    "        # TODO: Make use of self.action by passing these input observations into self.critic\n",
    "        # HINT: you'll want to populate the critic's obs_t_ph placeholder \n",
    "        if len(obs.shape) > 1:\n",
    "            observation = obs\n",
    "        else:\n",
    "            observation = obs[None]\n",
    "        return self.sess.run([self.action], feed_dict = {self.critic.obs_t_ph: obs})[0]\n",
    "\n",
    "\n",
    "class DQNAgent(object):\n",
    "    def __init__(self, sess, env, agent_params):\n",
    "\n",
    "        self.env = env\n",
    "        self.sess = sess\n",
    "        self.agent_params = agent_params\n",
    "        self.batch_size = agent_params['batch_size']\n",
    "        self.last_obs = self.env.reset()\n",
    "\n",
    "        self.num_actions = agent_params['ac_dim']\n",
    "        self.learning_starts = agent_params['learning_starts']\n",
    "        self.learning_freq = agent_params['learning_freq']\n",
    "        self.target_update_freq = agent_params['target_update_freq']\n",
    "\n",
    "        self.replay_buffer_idx = None\n",
    "        self.exploration = agent_params['exploration_schedule']\n",
    "        self.optimizer_spec = agent_params['optimizer_spec']\n",
    "\n",
    "        self.critic = DQNCritic(sess, agent_params, self.optimizer_spec)\n",
    "        self.actor = ArgMaxPolicy(sess, self.critic)\n",
    "\n",
    "        lander = agent_params['env_name'] == 'LunarLander-v2'\n",
    "        self.replay_buffer = MemoryOptimizedReplayBuffer(\n",
    "            agent_params['replay_buffer_size'], agent_params['frame_history_len'], lander=lander)\n",
    "        self.t = 0\n",
    "        self.num_param_updates = 0\n",
    "\n",
    "    def add_to_replay_buffer(self, paths):\n",
    "        pass\n",
    "\n",
    "    def step_env(self):\n",
    "\n",
    "        \"\"\"\n",
    "            Step the env and store the transition\n",
    "\n",
    "            At the end of this block of code, the simulator should have been\n",
    "            advanced one step, and the replay buffer should contain one more transition.\n",
    "\n",
    "            Note that self.last_obs must always point to the new latest observation.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO store the latest observation into the replay buffer\n",
    "        # HINT: see replay buffer's function store_frame\n",
    "        # get an index in buffer\n",
    "        self.replay_buffer_idx = self.replay_buffer.store_frame(self.last_obs)\n",
    "\n",
    "        eps = self.exploration.value(self.t)\n",
    "        # TODO use epsilon greedy exploration when selecting action\n",
    "        # HINT: take random action \n",
    "            # with probability eps (see np.random.random())\n",
    "            # OR if your current step number (see self.t) is less that self.learning_starts\n",
    "        perform_random_action = np.random.random() < eps or self.t < self.learning_starts\n",
    "\n",
    "        if perform_random_action:\n",
    "            action = np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            # TODO query the policy to select action\n",
    "            # HINT: you cannot use \"self.last_obs\" directly as input\n",
    "            # into your network, since it needs to be processed to include context\n",
    "            # from previous frames. \n",
    "            # Check out the replay buffer, which has a function called\n",
    "            # encode_recent_observation that will take the latest observation\n",
    "            # that you pushed into the buffer and compute the corresponding\n",
    "            # input that should be given to a Q network by appending some\n",
    "            # previous frames.\n",
    "            enc_last_obs = self.replay_buffer.encode_recent_observation()\n",
    "            enc_last_obs = enc_last_obs[None, :]\n",
    "\n",
    "            # TODO query the policy with enc_last_obs to select action\n",
    "            action = self.actor.get_action(enc_last_obs)\n",
    "            action = action[0]\n",
    "\n",
    "        # TODO take a step in the environment using the action from the policy\n",
    "        # HINT1: remember that self.last_obs must always point to the newest/latest observation\n",
    "        # HINT2: remember the following useful function that you've seen before:\n",
    "            #obs, reward, done, info = env.step(action)\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.last_obs = obs\n",
    "\n",
    "        # TODO store the result of taking this action into the replay buffer\n",
    "        # HINT1: see replay buffer's store_effect function\n",
    "        # HINT2: one of the arguments you'll need to pass in is self.replay_buffer_idx from above\n",
    "        self.replay_buffer.store_effect(self.replay_buffer_idx, action, reward, done)\n",
    "\n",
    "        # TODO if taking this step resulted in done, reset the env (and the latest observation)\n",
    "        if done:\n",
    "            obs = self.env.reset()\n",
    "            self.last_obs = obs\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if self.replay_buffer.can_sample(self.batch_size):\n",
    "            return self.replay_buffer.sample(batch_size)\n",
    "        else:\n",
    "            return [],[],[],[],[]\n",
    "\n",
    "    def train(self, ob_no, ac_na, re_n, next_ob_no, terminal_n):\n",
    "\n",
    "        \"\"\"\n",
    "            Here, you should train the DQN agent.\n",
    "            This consists of training the critic, as well as periodically updating the target network.\n",
    "        \"\"\"\n",
    "\n",
    "        loss = 0.0\n",
    "        if (self.t > self.learning_starts and \\\n",
    "                self.t % self.learning_freq == 0 and \\\n",
    "                self.replay_buffer.can_sample(self.batch_size)):\n",
    "\n",
    "            # TODO populate all placeholders necessary for calculating the critic's total_error\n",
    "            # HINT: obs_t_ph, act_t_ph, rew_t_ph, obs_tp1_ph, done_mask_ph\n",
    "            feed_dict = {\n",
    "                self.critic.learning_rate: self.optimizer_spec.lr_schedule.value(self.t),\n",
    "                self.critic.obs_t_ph: ob_no,\n",
    "                self.critic.act_t_ph: ac_na,\n",
    "                self.critic.rew_t_ph: re_n,\n",
    "                self.critic.obs_tp1_ph: next_ob_no,\n",
    "                self.critic.done_mask_ph: terminal_n,\n",
    "            }\n",
    "\n",
    "            # TODO: create a LIST of tensors to run in order to \n",
    "            # train the critic as well as get the resulting total_error\n",
    "            tensors_to_run = [self.critic.total_error, self.critic.train_fn]\n",
    "            loss, _ = self.sess.run(tensors_to_run, feed_dict=feed_dict)\n",
    "            # Note: remember that the critic's total_error value is what you\n",
    "            # created to compute the Bellman error in a batch, \n",
    "            # and the critic's train function performs a gradient step \n",
    "            # and update the network parameters to reduce that total_error.\n",
    "\n",
    "            # TODO: use sess.run to periodically update the critic's target function\n",
    "            # HINT: see update_target_fn\n",
    "            if self.num_param_updates % self.target_update_freq == 0:\n",
    "                _ = self.sess.run([self.critic.update_target_fn])\n",
    "\n",
    "            self.num_param_updates += 1\n",
    "\n",
    "        self.t += 1\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
